<!doctype html>
<html lang="en">

<!--
	ABSTRACT
	========

	Many tasks in quantum information rely on accurate knowledge of a system's Hamiltonian, including calibrating control, characterizing devices, and verifying quantum simulators. In this talk, we pose the problem of learning Hamiltonians as an instance of parameter estimation. We then solve this problem with Bayesian inference, and describe how rejection and particle filtering provide efficient numerical algorithms for learning Hamiltonians. Finally, we discuss how filtering can be combined with quantum resources to verify quantum systems beyond the reach of classical simulators.
-->

	<head>
		<meta charset="utf-8">

		<title>Rejection and Particle Filtering for Quantum Information</title>

		<meta name="author" content="Christopher E. Granade">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/cgranade-dark.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<style type="text/css" media="screen">
			.reveal section img {
				border: none;
				box-shadow: none;
				background: none;
			}

			.contrast-bg h3 {
				background: rgba(240, 241, 235, 255);
			}

			.hideable-foot {
				display: block;
				opacity: 0;
				font-size: 80% !important;
				position: absolute;
				bottom: 0em;
				right: 0.5em;
				text-align: right;
				transition: all 2s;
				z-index: 1;
			}

			.show-foot-pbt #foot-pbt,
			.show-foot-better #foot-better,
			.show-foot-title-foot #foot-title-foot,
			.show-foot-condensation #foot-condensation,
			.show-foot-qinfer #foot-qinfer {
				opacity: 1 !important;
				transition: all 2s;
				z-index: 2;
			}
		</style>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!--
				This is an awful hack. Don't use it if you can help it.
				See https://www.raymondcamden.com/2014/04/01/Adding-an-Absolutely-Positioned-Header-to-Revealjs/ for why it "works".
			-->
			<p class="hideable-foot" id="foot-title-foot">
				<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br>
				<cite class="doi"><a href="https://dx.doi.org/10/s87">10/s87</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/tf3">10/tf3</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/7nx">10/7nx</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/bk9d">10/bk9d</a></cite><br>
				<a href="http://www.cgranade.com/research/talks/quics/07-2016">
					www.cgranade.com/research/talks/quics/07-2016
				</a>
			</p>

			<p class="hideable-foot" id="foot-pbt">
				Granade, Combes, Cory <cite class="doi"><a href="https://dx.doi.org/10/bhdw">10/bhdw</a></cite>
			</p>

			<!-- <p class="hideable-foot" id="foot-better">
				Sergeevich <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/c4vv95">10/c4vv95</a></cite>,
				Ferrie <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tfx">10/tfx</a></cite>, 
				Hall and Wiseman <cite class="doi"><a href="https://dx.doi.org/10/bh6v">10/bh6v</a></cite>
			</p> -->

			<p class="hideable-foot" id="foot-condensation">
				Isard and Blake <cite class="doi"><a href="https://dx.doi.org/10/cc76f6">10/cc76f6</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qinfer">
				<a href="http://qinfer.org">qinfer.org</a>
			</p>

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-state="show-foot-title-foot">
					<h1 style="font-size: 175%">Rejection and Particle Filtering for Hamiltonian Learning</h1>
					<hr>
					<p>
						<a href="http://www.cgranade.com">Christopher E. Granade</a> <br>
						<a href="https://equs.org/">Centre for Engineered Quantum Systems</a>
                        $
							\newcommand{\ee}{\mathrm{e}}
							\newcommand{\ii}{\mathrm{i}}
							\newcommand{\dd}{\mathrm{d}}
							\newcommand{\id}{&#x1d7d9;}
							\newcommand{\TT}{\mathrm{T}}
							\newcommand{\defeq}{\mathrel{:=}}
							\newcommand{\Tr}{\operatorname{Tr}}
							\newcommand{\Var}{\operatorname{Var}}
							\newcommand{\Cov}{\operatorname{Cov}}
							\newcommand{\rank}{\operatorname{rank}}
							\newcommand{\expect}{\mathbb{E}}
							\newcommand{\sket}[1]{|#1\rangle\negthinspace\rangle}
							\newcommand{\sbraket}[1]{\langle\negthinspace\langle#1\rangle\negthinspace\rangle}
							\newcommand{\Gini}{\operatorname{Ginibre}}
							\newcommand{\supp}{\operatorname{supp}}
							\newcommand{\ket}[1]{\left|#1\right\rangle}
							\newcommand{\bra}[1]{\left\langle#1\right|}
							\newcommand{\braket}[1]{\left\langle#1\right\rangle}
						$
					</p>
					
					<p>
						joint work with<br>
						Nathan Wiebe ‚Ä¢ Christopher Ferrie ‚Ä¢ D. G. Cory
					</p>
				</section>

				<section>
					<p>
						Learning Hamiltonians is critical to a range of QIP tasks:
					</p>

					<dl>
						<dt>Calibration</dt>
						<dd>Static field / pulse power / crosstalk, etc.</dd>

						<dt>Debugging/Diagnosis</dt>
						<dd>$T_2$ estimation, other noise finding</dd>

						<dt>Verification/Validation</dt>
						<dd>Analog and digital quantum simulation</dd>
					</dl>
				</section>

				<section data-markdown>
					### **Example**: Ramsey Estimation ###

					Suppose $H = \omega \sigma_z / 2$ for some unknown $\omega$.

					Traditional approach:

					- Prepare $\ket{+} \propto \ket{0} + \ket{1}$, measure ‚Äúclick‚Äù w/ pr.:
					$
						\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2 = \cos^2(\omega t / 2)
					$.
					- Repeat for many ‚Äúshots‚Äù to estimate click pr.
					- Repeat for many times to estimate signal.
				</section>

				<section>
					You'll get something that looks a bit like this:

					<img src="figures/rabi-example-signal.png" width="90%">
				</section>

				<section>
					What's $\omega$? Fourier transform and look at the peak.

					<img src="figures/rabi-example-spectrum.png" width="90%">
				</section>

				<section data-markdown>
					We can do better.

					# $H = H(\vec{x})$. # 

					Hamiltonian learning is a special case of *parameter estimation*:
					given data $D$, what is $\vec{x}$?
				</section>

				<section data-markdown>
					### The Likelihood Function ###

					$\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2$
					  defines probability $\Pr(d | \omega; t)$ for every
					  outcome $d$, model $\omega$ and experiment $t$.

					Basis for both maximum-likelihood and Bayesian methods.
				</section>

				<section data-markdown>
					### Bayesian Parameter Estimation ###

					The likelihood tells us what we learn from data:

					$$
						\Pr(\vec{x} | d; e) = \frac{\Pr(d | \vec{x}; e)}{\Pr(d | e)} \Pr(\vec{x})
					$$

					---

					Estimate $\hat{x} = \expect[\vec{x} | d; e] = \int \vec{x} \Pr(\vec{x} | d; e)\dd \vec{x}$.

					- **Optimal** for mean-squared error.
				</section>

				<section data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/bayesian-pe-flowchart.png">
					
				</section>

				<section data-markdown>
					Each posterior $\Pr(\vec{x} | d; e)$ encodes our uncertainty about
					$x$.

					$$
						\expect[(\hat{x} - \vec{x})^\TT (\hat{x} - \vec{x})] = \Tr \Cov(\vec{x})
					$$

					---

					Can use to make *adaptive* decisions:
					$$
						e_* = \operatorname{arg min}_e \expect_d[\Tr \Cov(\vec{x} | d; e)]
					$$
				</section>

				<section data-markdown>
					### **Example**: $x = (\omega)$ ###

					Can analytically find posterior for Gaussian priors,
					use to adaptively choose $t_k$.

					![](./figures/how-t2.png)
				</section>

				<section>
					<p data-markdown>
						**Problem**: exactly finding $\hat{x}$ is intractable
						in general.
					</p>

					<p class="fragment" data-markdown>
						**Answer**: numerically approximate $\int f(\vec{x}) \Pr(\vec{x} | d)\dd\vec{x}$.
					</p>
				</section>

				<section data-markdown>
					## Monte Carlo Integration ##

					$$
						\int f(\vec{x}) p(\vec{x})\dd\vec{x} \approx \frac{1}{N} \sum_i f(\vec{x}_i)
						\text{ for } \left\\{\vec{x}_i\right\\} \sim p(\vec{x})
					$$

					---

					Efficient if we can sample from $p(\vec{x})$.

					How do we do that for $p(\vec{x}) = \Pr(\vec{x} | d; e)$?
				</section>

				<section data-markdown>
					## Rejection Sampling ##

					Given samples from $\Pr(\vec{x})$ and likelihood
					function $\Pr(d | \vec{x}; e)$, how do we sample
					from posterior for datum $d$?

					- $\vec{x} \sim \Pr(\vec{x})$.
					- $u \sim \operatorname{Uniform}(0, 1)$.
					- If $u \le \Pr(d | \vec{x}; e)$, yield sample $\vec{x}$.
					  Otherwise, reject $\vec{x}$.

					Accepted samples are distributed according to posterior.
				</section>

				<section>
					<p data-markdown>
						**Next problem**: likelihood $\to 0$ with large data
						sets.
					</p>

					<p class="fragment" data-markdown>
						**Next answer**: *resample* instead of drawing
						from initial prior.
					</p>
				</section>

				<section data-markdown>
					After each datum $d$, match moments to a Gaussian, draw new samples
					from that Gaussian.

					- $\bar{x} \gets \expect[\vec{x} | d]$.
					- $\Sigma \gets \operatorname{Cov}[\vec{x} | d] = \expect[\vec{x} \vec{x}^\TT | d] - \bar{x} \bar{x}^\TT$.
				</section>

				<section data-markdown>
					## Welford's Algorithm ##
					
					Can compute $\bar{x}$, $\Sigma$ from
					one sample at a time. Numerically stable.

					- $n, \bar{x}, M_2 \gets 0$.
					- **For** each sample $x$:
						- $n \gets n + 1$
						- $\Delta \gets x - \mu$
						- $\bar{x} \gets \bar{x} + \Delta / n$
						- $M_2 \gets M_2 + \Delta (x - \bar{x})$
					- **Return** mean $\bar{x}$, variance $M_2 / (n - 1)$.

					Vector case is similar.
				</section>

				<section data-markdown>
					## Rejection Filtering (ReJF) ##

					**Input**: Prior mean $\bar{x}$, prior covariance $\Sigma$,
						number of attempts $m$.

					- **For** each datum $d$ and experiment $e$:
						- $n, \bar{x}', M_2 \gets 0$
						- **While** $n < m$:
							- $\vec{x} \sim \mathcal{N}(\bar{x}, \Sigma)$.
							- Accept $\vec{x}$ w/ $\Pr(d | \vec{x}; e)$.
							- **If** accepted, update $n$, $\bar{x}'$, $M_2$.
						- $\bar{x} \gets \bar{x}'$, $\Sigma \gets M_2 / (n - 1)$.
				</section>

				<section data-markdown>
					## Advantages of RejF ##

					- Easy to implement
					- Never needed to remember each accepted $x$!
						- Very low-memory (constant # of
						  accumulator registers), ideal for FPGA use.
					- Easily parallelizable
				</section>

				<section>
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<p>
						Prepare state $\ket{\phi}$ s. t. $U\ket{\phi} = \ee^{\ii \phi}\ket{\phi}$,
						measure to learn $\phi$.
					</p>

					<img src="./figures/pe-circuit-crop.png">

					<p>
						If $\theta = 0$, same likelihood as Ramsey/Rabi, with $\phi = \omega$, $M = t$.
					</p>


					<!-- show rfpe results, connect pe problem back to larmor -->
				</section>


				<section>
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<img src="./figures/pe-error.png" width="80%">
				</section>

				<section>
					<p  data-markdown>
						**Drawback**: RejF requires posterior after each datum
						to be $\approx$ Gaussian.
					</p>

					<p class="fragment" data-markdown>
						We can solve this by using a more general approach
						to resampling and filtering.
					</p>
				</section>

				<section data-markdown>
					## Liu-West Resampler ##

					If we remember each sample $\vec{x}$, we can use them to
					relax RejF assumptions.

					To draw new $\vec{x}'$ from $p(\vec{x})$, let
					$a^2 + h^2 = 1$, then:

					- Approximate $\bar{x} \gets \expect[\vec{x}]$, $\Sigma \gets \operatorname{Cov}(\vec{x})$.
					- Draw $\vec{x}$ from original approx of $p(\vec{x})$.
					- Draw $\vec{\epsilon} \sim \mathcal{N}(0, h \Sigma)$.
					- **Return** $\vec{x}' \gets a \vec{x} + (1 - a) \bar{x} + \vec{\epsilon}$.

					---

					- $a \to 0$: RejF (assumed density) approx
					- $a \to 1$: Bootstrap
				</section>

				<section>
					<p>
						Resampling provides numerical stability
						with less particles.
					</p>

					<img src="figures/impovrishment.png">
				</section>

				<section data-markdown>
					## Particle Filter ##

					- Assign a *weight* $w_i$ to each sample $\vec{x}_i$.
					- Instead of rej. sampling, update weights by
					$$
					    w_i \mapsto w_i \times \Pr(d | \vec{x}_i; e).
					$$
					- Renormalize (set $\sum_i w_i = 1$).
					- Periodically use Liu-West to draw new $\\{\vec{x}_i\\}$.

					---

					Corresponds to
					$
						p(\vec{x}) \approx \sum_i w_i \delta(\vec{x} - \vec{x}_i).
					$
				</section>


				<!-- <section
					class="light-figure"
					data-background="#fff"
					data-background-image='figures/impovrishment.png'
				    data-background-position="center"
				    data-background-repeat="no-repeat"
					data-background-size="90%">
				</section> -->

				<!-- <section data-markdown>
					### **Example**: $x = (\omega)$, known $T_2 &lt; \infty$ ###


				</section> -->

				<section data-markdown>
					### **Example**: $x = (\omega, T_2)$ ###

					![](figures/unknown-t2.png)
				</section>

				<section data-markdown data-state="show-foot-qinfer">
					#### **QInfer**: Particle Filter Implementation for Quantum Info ####

					We provide an open-source implementation for use in experiment and theory alike.
				</section>

				<section data-markdown>
					Useful for Hamiltonian models...

					- Rabi/Ramsey/Phase est. (todo)
					- Swap spectroscopy (todo)

					...as well as other QIP tasks.

					- Tomography (todo)
					- RB (todo)
					- Homodyne meas (todo)
				</section>

				<section data-markdown>
					### Limitations of Particle Filtering ###

					- Requires classically simulating likelihoods
					  for each of many (~ks) particles.

					Let's do better: use *quantum* simulation instead.
				</section>

				<section data-markdown>
					### Two Kinds of Simulation ###

					![](figures/simulators.png)
				</section>

				<section data-markdown>
					### Likelihood-Free RejF ###

					Replace rej. sampling step by drawing
					datum from likelihood instead of computing
					exact value:

					- Draw datum $d'$ from $\Pr(d | \vec{x}; e)$.
					- Accept $\vec{x}$ if $d = d'$.
				</section>

				<section data-markdown>
					### Likelihood-Free Particle Filtering ###

					Can also use weak simulation to approximate likelihoods
					in particle filtering.
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak-desaturated.png"
						 >
					#### **Example**: Noisy Coin ####

					How well can we learn the bias $x = (p)$ of a noisy coin?

					$$
						\Pr(\text{click} | p) = 0.95 p + 0.1 (1 - p)
					$$
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak.png"
						 data-background-transition="fade-in"
						 data-transition="fade-in">
				</section>

				<section data-markdown>
					### Quantum Hamiltonian Learning ###

					![](figures/qle.png)
				</section>

				<section data-markdown>
					### **Example**: Nearest-Neighbor 1D Ising ###

					![](figures/qle-line-ising.png)
				</section>

				<section data-markdown>
					We can do more with access to a *trusted*
					simulator.


					### Quantum Interactivity ### 
					
					![](figures/iqle.png)
				</section>

				<section>
					<p>
						We design experiments using the
					</p>

					<h3><strong>PGH</strong>: Particle Guess Heuristic</h3>

					<ul>
						<li>
							Draw $\vec{x}_-, \vec{x}_-'$ from current
					  		posterior.
					  	</li>
						<li>Let $t = 1 / |\vec{x}_- - \vec{x}_-'|$.</li>
						<li><strong>Return</strong> $e = (\vec{x}_-, t)$.</li>
					</ul>
				</section>

				<section data-markdown>
					### **Example**: Ising on Complete Graph ###

					![](figures/iqle-complete-ising.png)
				</section>

				<section data-markdown>
					Robust even to wrong model. ($0.5$ NN + $10^{-4}$ Complete)

					![](figures/qhl-bad-model.png)
				</section>

				<section>
					<p>
						One important approximation: physical locality.
					</p>

					<img src="./figures/qbs-lightcones.png" width="65%">

					<p>
						Approximation quality can be bounded if
						Lieb-Robinson velocity is finite.
					</p>
				</section>

				<section>
					<p>
						Scan trusted device across untrusted.
					</p>

					<img src="./figures/scanning.png" width="55%">

					<p>
						Run particle filter <em>only</em> on supported
						parameters.
					</p>
				</section>

				<section>
					<h4>50 qubit Ising chain, 8 qubit simulator, 4 qubit observable</h4>

					<img src="./figures/qbs-error-per-scan.png" width="60%">
				</section>

				<section data-markdown>
					### Going Further ###

					- Time-dependence
					- Model selection / structured models
					- Quantum algorithms for filtering
				</section>

				<section data-markdown>
					# üíñ Thank you! üíñ #
				</section>

				<section data-background="#fff" data-background-video="figures/multicos-distributions.mp4">					
				</section>


			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: true,
				zoomKey: 'shift',

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/0.9.16/socket.io.min.js', async: true },
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
