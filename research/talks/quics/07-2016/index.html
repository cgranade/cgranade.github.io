<!doctype html>
<html lang="en">

<!--
	ABSTRACT
	========

	Many tasks in quantum information rely on accurate knowledge of a system's Hamiltonian, including calibrating control, characterizing devices, and verifying quantum simulators. In this talk, we pose the problem of learning Hamiltonians as an instance of parameter estimation. We then solve this problem with Bayesian inference, and describe how rejection and particle filtering provide efficient numerical algorithms for learning Hamiltonians. Finally, we discuss how filtering can be combined with quantum resources to verify quantum systems beyond the reach of classical simulators.
-->

	<head>
		<meta charset="utf-8">

		<title>Rejection and Particle Filtering for Quantum Information</title>

		<meta name="author" content="Christopher E. Granade">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/cgranade-dark.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<style type="text/css" media="screen">
			.hideable-foot {
				display: block;
				opacity: 0;
				font-size: 80% !important;
				position: absolute;
				bottom: 0em;
				right: 0.5em;
				text-align: right;
				transition: all 2s;
				z-index: 1;
			}

			.show-foot-pbt #foot-pbt,
			.show-foot-how #foot-how,
			.show-foot-lfpe #foot-lfpe,
			.show-foot-lw #foot-lw,
			.show-foot-smc #foot-smc,
			.show-foot-better #foot-better,
			.show-foot-title-foot #foot-title-foot,
			.show-foot-condensation #foot-condensation,
			.show-foot-qinfer #foot-qinfer,
			.show-foot-rfpe #foot-rfpe {
				opacity: 1 !important;
				transition: all 2s;
				z-index: 2;
			}
		</style>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!--
				This is an awful hack. Don't use it if you can help it.
				See https://www.raymondcamden.com/2014/04/01/Adding-an-Absolutely-Positioned-Header-to-Revealjs/ for why it "works".
			-->
			<p class="hideable-foot" id="foot-title-foot">
				<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br>
				<cite class="doi"><a href="https://dx.doi.org/10/s87">10/s87</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/tf3">10/tf3</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/7nx">10/7nx</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/bk9d">10/bk9d</a></cite><br>
				<a href="http://www.cgranade.com/research/talks/quics/07-2016">
					www.cgranade.com/research/talks/quics/07-2016
				</a>
			</p>

			<p class="hideable-foot" id="foot-how">
				Ferrie, Granade, Cory <cite class="doi"><a href="https://dx.doi.org/10/tfx">10/tfx</a></cite>
			</p>

			<p class="hideable-foot" id="foot-pbt">
				Granade, Combes, Cory <cite class="doi"><a href="https://dx.doi.org/10/bhdw">10/bhdw</a></cite>
			</p>

			<p class="hideable-foot" id="foot-rfpe">
				Wiebe and Granade <cite class="doi"><a href="https://dx.doi.org/10/bk9d">10/bk9d</a></cite>
			</p>

			<p class="hideable-foot" id="foot-lw">
				Liu and West <cite class="doi"><a href="https://dx.doi.org/10/8c2">10/8c2</a></cite>
			</p>

			<p class="hideable-foot" id="foot-smc">
				Doucet <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/bmch">10/bmch</a></cite>
			</p>

			<p class="hideable-foot" id="foot-lfpe">
				Ferrie and Granade <cite class="doi"><a href="https://dx.doi.org/10/tdj">10/tdj</a></cite>
			</p>


			<!-- <p class="hideable-foot" id="foot-better">
				Sergeevich <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/c4vv95">10/c4vv95</a></cite>,
				Ferrie <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tfx">10/tfx</a></cite>, 
				Hall and Wiseman <cite class="doi"><a href="https://dx.doi.org/10/bh6v">10/bh6v</a></cite>
			</p> -->

			<p class="hideable-foot" id="foot-condensation">
				Isard and Blake <cite class="doi"><a href="https://dx.doi.org/10/cc76f6">10/cc76f6</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qinfer">
				<a href="http://qinfer.org">qinfer.org</a>
			</p>

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-state="show-foot-title-foot">
					<h1 style="font-size: 175%">Rejection and Particle Filtering for Hamiltonian Learning</h1>
					<hr>
					<p>
						<a href="http://www.cgranade.com">Christopher E. Granade</a> <br>
						<a href="https://equs.org/">Centre for Engineered Quantum Systems</a>
                        $
							\newcommand{\ee}{\mathrm{e}}
							\newcommand{\ii}{\mathrm{i}}
							\newcommand{\dd}{\mathrm{d}}
							\newcommand{\id}{&#x1d7d9;}
							\newcommand{\TT}{\mathrm{T}}
							\newcommand{\defeq}{\mathrel{:=}}
							\newcommand{\Tr}{\operatorname{Tr}}
							\newcommand{\Var}{\operatorname{Var}}
							\newcommand{\Cov}{\operatorname{Cov}}
							\newcommand{\rank}{\operatorname{rank}}
							\newcommand{\expect}{\mathbb{E}}
							\newcommand{\sket}[1]{|#1\rangle\negthinspace\rangle}
							\newcommand{\sbraket}[1]{\langle\negthinspace\langle#1\rangle\negthinspace\rangle}
							\newcommand{\Gini}{\operatorname{Ginibre}}
							\newcommand{\supp}{\operatorname{supp}}
							\newcommand{\ket}[1]{\left|#1\right\rangle}
							\newcommand{\bra}[1]{\left\langle#1\right|}
							\newcommand{\braket}[1]{\left\langle#1\right\rangle}
						$
					</p>
					
					<p>
						joint work with<br>
						Nathan Wiebe • Christopher Ferrie • D. G. Cory
					</p>
				</section>

				<section>
					<p>
						Learning Hamiltonians is critical to a range of QIP tasks:
					</p>

					<dl>
						<dt>Calibration</dt>
						<dd>Static field / pulse power / crosstalk, etc.</dd>

						<dt>Debugging/Diagnosis</dt>
						<dd>$T_2$ estimation, other noise finding</dd>

						<dt>Verification/Validation</dt>
						<dd>Analog and digital quantum simulation</dd>
					</dl>
				</section>

				<section data-markdown>
					### **Example**: Ramsey Estimation ###

					Suppose $H = \omega \sigma_z / 2$ for some unknown $\omega$.

					Traditional approach:

					- Prepare $\ket{+} \propto \ket{0} + \ket{1}$, measure “click” w/ pr.:
					$
						\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2 = \cos^2(\omega t / 2)
					$.
					- Repeat for many “shots” to estimate click pr.
					- Repeat for many times to estimate signal.
				</section>

				<section>
					You'll get something that looks a bit like this:

					<img src="figures/rabi-example-signal.png" width="90%">
				</section>

				<section>
					What's $\omega$? Fourier transform and look at the peak.

					<img src="figures/rabi-example-spectrum.png" width="90%">
				</section>

				<section data-markdown>
					We can do better.

					# $H = H(\vec{x})$. # 

					Hamiltonian learning is a special case of *parameter estimation*:
					given data $D$, what is $\vec{x}$?
				</section>

				<section data-markdown>
					### The Likelihood Function ###

					$\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2$
					  defines probability $\Pr(d | \omega; t)$ for every
					  outcome $d$, model $\omega$ and experiment $t$.

					Basis for both maximum-likelihood and Bayesian methods.
				</section>

				<section data-markdown>
					### Bayesian Parameter Estimation ###

					The likelihood tells us what we learn from data:

					$$
						\Pr(\vec{x} | d; e) = \frac{\Pr(d | \vec{x}; e)}{\Pr(d | e)} \Pr(\vec{x})
					$$

					---

					Estimate $\hat{x} = \expect[\vec{x} | d; e] = \int \vec{x} \Pr(\vec{x} | d; e)\dd \vec{x}$.

					- **Optimal** for mean-squared error.
				</section>

				<section data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/bayesian-pe-flowchart.png">
					
				</section>

				<section data-markdown>
					Each posterior $\Pr(\vec{x} | d; e)$ encodes our uncertainty about
					$x$.

					**Scalar case**: let $\delta = x - \hat{x}$ be error in $\hat{x}$.
					What should we expect squared-error to be?

					\begin{align}
						\expect[\delta^2 | d; e] &amp; = \int \delta^2 \Pr(\vec{x} | d; e) \dd\vec{x} \\\\
			            &amp; = \expect[(x - \hat{x})^2 | d; e] \\\\
                        &amp; = \expect[x^2 | d; e]  - 2 \expect[x| d; e] \hat{x} + \hat{x}^2 \\\\
                        &amp; = \expect[x^2 | d; e]  - \hat{x}^2.
					\end{align}

					This is the **variance** $\mathbb{V}[x \mid d; e]$.

					**Vector case**: $\Cov(\vec{x} | d; e) \defeq \expect[(\vec{x} - \hat{x}) (\vec{x} - \hat{x})^\TT | d; e]$.
				</section>

				<section data-markdown>
					We can use our posteriors to make *adaptive* decisions:
					$$
						e_* = \operatorname{arg min}_e \expect_d[\mathbb{V}(x | d; e)]
					$$
				</section>

				<section data-markdown data-state="show-foot-how">
					### **Example**: $x = (\omega)$ ###

					Can analytically find posterior for Gaussian priors,
					use to adaptively choose $t_k$.

					![](./figures/how-t2.png)
				</section>

				<section>
					<p data-markdown>
						**Problem**: exactly finding $\hat{x}$ is intractable
						in general.
					</p>

					<p class="fragment" data-markdown>
						**Answer**: numerically approximate $\int f(\vec{x}) \Pr(\vec{x} | d)\dd\vec{x}$.
					</p>
				</section>

				<section data-markdown>
					## Monte Carlo Integration ##

					$$
						\int f(\vec{x}) p(\vec{x})\dd\vec{x} \approx \frac{1}{N} \sum_i f(\vec{x}_i)
						\text{ for } \left\\{\vec{x}_i\right\\} \sim p(\vec{x})
					$$

					---

					Efficient *if* we can sample from $p(\vec{x})$.

					How do we do that for $p(\vec{x}) = \Pr(\vec{x} | d; e)$?
				</section>

				<section data-markdown>
					## Rejection Sampling ##

					Given samples from $\Pr(\vec{x})$ and likelihood
					function $\Pr(d | \vec{x}; e)$, how do we sample
					from posterior for datum $d$?

					- $\vec{x} \sim \Pr(\vec{x})$.
					- $u \sim \operatorname{Uniform}(0, 1)$.
					- If $u \le \Pr(d | \vec{x}; e)$, yield sample $\vec{x}$.
					  Otherwise, reject $\vec{x}$.

					Accepted samples are distributed according to posterior.
				</section>

				<section>
					<h3>Rejection Sampling Isn't Enough</h3>

					<p>
						Let $D = {d_1, \dots, d_k}$ be a set of data.<br>
						A prior sample $\vec{x}$ is accepted with
					</p>
					<p>
						$$
							\Pr(D | \vec{x}) = \prod_{d \in D} \Pr(d | \vec{x})
							\overset{k \to \infty}{\longrightarrow} 0.
						$$
					</p>
				</section>

				<section>
					<h3><strong>Example:</strong> Biased Coin $x = (p)$</h3>
					<p>
						Consider $\Pr(H | p) = p$, $d \in \{H, T\}$.
					</p>

					<p>
						If $p \approx 0.5$, $\Pr(d_1, \dots, d_k | p) \approx 1 / 2^k$.
					</p>

					<p>
						We will reject all but exponentially few samples!
					</p>

					<div class="fragment">
						<hr>

						<p data-markdown>
							**Solution**: draw new samples
							after each datum instead of rejecting with the full data set at once.
						</p>
					</div>
				</section>

				<section data-markdown>
					### Gaussian Resampling ###

					For each datum $d$, use rejection sampling to approximate posterior
					moments:

					- $\bar{x} \gets \expect[\vec{x} | d]$.
					- $\Sigma \gets \operatorname{Cov}[\vec{x} | d] = \expect[\vec{x} \vec{x}^\TT | d] - \bar{x} \bar{x}^\TT$.

					---

					At the next iteration, draw prior samples from Gaussian with these moments:
					$$
						\vec{x} | d \sim \mathcal{N}(\bar{x}, \Sigma)
					$$
				</section>

				<section>
					<h2>Welford's Algorithm</h2>
					
					<p>
						Can compute $\bar{x}$, $\Sigma$ from
						one sample at a time. Numerically stable.
					</p>

					<ul style="width: 80%">
						<li>$n, \bar{x}, M_2 \gets 0$.</li>
						<li><strong>For</strong> each sample $x$:</li>
						<ul>
							<li>
								$n \gets n + 1$
								<span class="comment">Record # of samples</span>
							</li>
							<li>
								$\Delta \gets x - \mu$
								<span class="comment">Diff to running mean</span>
							</li>
							<li>
								$\bar{x} \gets \bar{x} + \Delta / n$
								<span class="comment">Update running mean</span>
							</li>
							<li>
								$M_2 \gets M_2 + \Delta (x - \bar{x})$
								<span class="comment">Update running var</span>
							</li>
							<li><strong>Return</strong> mean $\bar{x}$, variance $M_2 / (n - 1)$.</li>
						</ul>
					</ul>

					<p>
						Vector case is similar.
					</p>
				</section>

				<section data-state="show-foot-rfpe">
					<h2>Rejection Filtering (ReJF)</h2>

					<p>
					    <strong>Input:</strong>
					    Prior mean $\bar{x}$, prior covariance $\Sigma$,
						number of attempts $m$.
					<p>

					<ul style="width: 80%">
						<li><strong>For</strong> each datum $d$ and experiment $e$:</li>
						<ul>
							<li>
								$n, \bar{x}', M_2 \gets 0$
								<span class="comment">Initialize Welford.</span>
							</li>
							<li><strong>While</strong> $n < m$:</li>
							<ul>
								<li>Draw $\vec{x} \sim \mathcal{N}(\bar{x}, \Sigma)$.</li>
								<li>Accept $\vec{x}$ w/ $\Pr(d | \vec{x}; e)$.</li>
								<li><strong>If</strong> accepted, update $n$, $\bar{x}'$, $M_2$.</li>
							</ul>
							<li>
								$\bar{x} \gets \bar{x}'$, $\Sigma \gets M_2 / (n - 1)$.
								<span class="comment">
									Est. moments.
								</span>
							</li>
						</ul>
					</ul>
				</section>

				<section data-markdown>
					## Advantages of RejF ##

					- Accept pr based on a single datum
					  ⇒ likelihood $\not\to 0$.
					- Easy to implement
					- Never needed to remember each accepted $x$!
						- Very low-memory (constant # of
						  registers), ideal for FPGA use.
					- Easily parallelizable
				</section>

				<section>
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<p>
						Prepare state $\ket{\phi}$ s. t. $U\ket{\phi} = \ee^{\ii \phi}\ket{\phi}$,
						measure to learn $\phi$.
					</p>

					<img src="./figures/pe-circuit-crop.png">

					<p>
						If $\theta = 0$, same likelihood as Ramsey/Rabi, with $\phi = \omega$, $M = t$.
					</p>


					<!-- show rfpe results, connect pe problem back to larmor -->
				</section>


				<section data-state="show-foot-rfpe">
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<img src="./figures/pe-error.png" width="80%">
				</section>

				<section>
					<p  data-markdown>
						**Drawback**: RejF requires posterior after each datum
						to be $\approx$ Gaussian.
					</p>

					<p class="fragment" data-markdown>
						We can solve this by using a more general approach:

						- Weaken Gaussian assumption.
						- Generalize the rejection sampling step.
					</p>
				</section>

				<section data-state="show-foot-lw">
					<h2>Liu-West Resampler</h2>

					<p>
						If we remember each sample $\vec{x}$, we can use them to
						relax RejF assumptions.
					</p>

					<p>
						<strong>Input:</strong>
						$a, h \in [0, 1]$ s.t. $a^2 + h^2 = 1$,
						distribution $p(\vec{x})$.
					</p>

					<ul>
						<li>
							Approximate $\bar{x} \gets \expect[\vec{x}]$, $\Sigma \gets \operatorname{Cov}(\vec{x})$
						</li>
						<li>Draw <em>parent</em> $\vec{x}$ from $p(\vec{x})$.</li>
						<li>Draw $\vec{\epsilon} \sim \mathcal{N}(0, \Sigma)$.</li>
						<li>
							<strong>Return</strong>
							new sample $\vec{x}' \gets a \vec{x} + (1 - a) \bar{x} + h \vec{\epsilon}$.
						</li>
					</ul>
				</section>

				<section data-markdown>
					### Why Does Liu-West Work? ###

					\begin{align}
						\vec{x}'          &amp; \gets a \vec{x} + (1 - a) \bar{x} + h \vec{\epsilon} \\\\
						\expect[\vec{x}'] &amp; = [a + (1 - a)] \bar{x} \\\\
						\Cov(\vec{x}')    &amp; = (a^2 + h^2) \Cov(\vec{x}). \\\\
						\Longrightarrow a^2 + h^2 &amp; = 1 \text{ preserves } \expect[\vec{x}], \Cov(\vec{x}).
					\end{align}

					---

					Mixes original approximation with $1 - a$ of a Gaussian
					matching moments.

					- $a \to 0$: RejF (assumed density) approx
					- $a \to 1$: Bootstrap
					- $a = 0.98$: typical case (2% Gaussian).
				</section>

				<section data-markdown>
					### From Samples to Particles ###

					Define a particle $(w_i, \vec{x}_i)$ as
					a sample $\vec{x}_i$ and a weight $w_i \in [0, 1]$.

					- $\expect[\vec{x}] = \sum_i w_i \vec{x}_i$
					- $\Cov(\vec{x}) =
					  \sum_i w_i \vec{x}_i \vec{x}_i^\TT - \expect[\vec{x}]\expect^\TT[\vec{x}]$

					- $\expect[f(\vec{x})] = \sum_i w_i f(\vec{x}_i)$

					---

					Corresponds to
					$
						p(\vec{x}) \approx \sum_i w_i \delta(\vec{x} - \vec{x}_i).
					$
				</section>

				<section data-markdown data-state="show-foot-smc">
					## Particle Filter ##

					- Draw $N$ initial samples $\vec{x}_i$ from the prior $\Pr(\vec{x})$.
					- Assign initial weights $w_i = 1 / N$ to be uniform
					  over each sample $\vec{x}_i$.
					- Instead of rej. sampling, update weights by
					$$
					    w_i \mapsto w_i \times \Pr(d | \vec{x}_i; e).
					$$
					- Renormalize (set $\sum_i w_i = 1$).
					- Periodically use Liu-West to draw new $\\{\vec{x}_i\\}$
					  with uniform weights.
				</section>

				<section>
					<p>
						Resampling provides numerical stability
						with less particles.
					</p>

					<img src="figures/impovrishment.png">
				</section>


				<!-- <section
					class="light-figure"
					data-background="#fff"
					data-background-image='figures/impovrishment.png'
				    data-background-position="center"
				    data-background-repeat="no-repeat"
					data-background-size="90%">
				</section> -->

				<!-- <section data-markdown>
					### **Example**: $x = (\omega)$, known $T_2 &lt; \infty$ ###


				</section> -->

				<section data-markdown>
					### **Example**: $x = (\omega, T_2)$ ###

					![](figures/unknown-t2.png)
				</section>

				<section data-markdown data-state="show-foot-qinfer">
					#### **QInfer**: Particle Filter Implementation for Quantum Info ####

					We provide an open-source implementation for use in experiment and theory alike.
				</section>

				<section data-markdown>
					Useful for Hamiltonian models...

					- Rabi/Ramsey/Phase est. (todo)
					- Swap spectroscopy (todo)

					...as well as other QIP tasks.

					- Tomography (todo)
					- RB (todo)
					- Homodyne meas (todo)
				</section>

				<section data-markdown>
					### Limitations of Particle Filtering ###

					- Requires classically simulating likelihoods
					  for each of many (~ks) particles.

					Let's do better: use *quantum* simulation instead.
				</section>

				<section data-markdown>
					### Two Kinds of Simulation ###

					![](figures/simulators.png)
				</section>

				<section data-markdown>
					### Likelihood-Free RejF ###

					Replace rej. sampling step by drawing
					datum from likelihood instead of computing
					exact value:

					- Draw datum $d'$ from $\Pr(d | \vec{x}; e)$.
					- Accept $\vec{x}$ if $d = d'$.
				</section>

				<section data-markdown data-state="show-foot-lfpe">
					### Likelihood-Free Particle Filtering ###

					Can also use weak simulation to approximate likelihoods
					in particle filtering.
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak-desaturated.png"
						 >
					#### **Example**: Noisy Coin ####

					How well can we learn the bias $x = (p)$ of a noisy coin?

					$$
						\Pr(\text{click} | p) = 0.95 p + 0.1 (1 - p)
					$$
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak.png"
						 data-background-transition="fade-in"
						 data-transition="fade-in">
				</section>

				<section data-markdown>
					### Quantum Hamiltonian Learning ###

					![](figures/qle.png)
				</section>

				<section data-markdown>
					### **Example**: Nearest-Neighbor 1D Ising ###

					![](figures/qle-line-ising.png)
				</section>

				<section data-markdown>
					We can do more with access to a *trusted*
					simulator.


					### Quantum Interactivity ### 
					
					![](figures/iqle.png)
				</section>

				<section>
					<p>
						We design experiments using the
					</p>

					<h3><strong>PGH</strong>: Particle Guess Heuristic</h3>

					<ul>
						<li>
							Draw $\vec{x}_-, \vec{x}_-'$ from current
					  		posterior.
					  	</li>
						<li>Let $t = 1 / |\vec{x}_- - \vec{x}_-'|$.</li>
						<li><strong>Return</strong> $e = (\vec{x}_-, t)$.</li>
					</ul>
				</section>

				<section data-markdown>
					### **Example**: Ising on Complete Graph ###

					![](figures/iqle-complete-ising.png)
				</section>

				<section data-markdown>
					Robust even to wrong model. ($0.5$ NN + $10^{-4}$ Complete)

					![](figures/qhl-bad-model.png)
				</section>

				<section>
					<p>
						One important approximation: physical locality.
					</p>

					<img src="./figures/qbs-lightcones.png" width="65%">

					<p>
						Approximation quality can be bounded if
						Lieb-Robinson velocity is finite.
					</p>
				</section>

				<section>
					<p>
						Scan trusted device across untrusted.
					</p>

					<img src="./figures/scanning.png" width="55%">

					<p>
						Run particle filter <em>only</em> on supported
						parameters.
					</p>
				</section>

				<section>
					<h4>50 qubit Ising chain, 8 qubit simulator, 4 qubit observable</h4>

					<img src="./figures/qbs-error-per-scan.png" width="60%">
				</section>

				<section data-markdown>
					### Going Further ###

					- Time-dependence
					- Model selection / structured models
					- Quantum algorithms for filtering
				</section>

				<section data-markdown>
					# 💖 Thank you! 💖 #
				</section>

				<section data-background="#fff" data-background-video="figures/multicos-distributions.mp4">					
				</section>


			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: true,
				zoomKey: 'shift',

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/0.9.16/socket.io.min.js', async: true },
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
