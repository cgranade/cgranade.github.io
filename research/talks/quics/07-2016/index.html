<!doctype html>
<html lang="en">

<!--
	ABSTRACT
	========

	Many tasks in quantum information rely on accurate knowledge of a system's Hamiltonian, including calibrating control, characterizing devices, and verifying quantum simulators. In this talk, we pose the problem of learning Hamiltonians as an instance of parameter estimation. We then solve this problem with Bayesian inference, and describe how rejection and particle filtering provide efficient numerical algorithms for learning Hamiltonians. Finally, we discuss how filtering can be combined with quantum resources to verify quantum systems beyond the reach of classical simulators.
-->

	<head>
		<meta charset="utf-8">

		<title>Rejection and Particle Filtering for Quantum Information</title>

		<meta name="author" content="Cassandra E. Granade">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/cgranade-dark.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<style type="text/css" media="screen">
			/*
				Calibration to local projector.
				Largely, this consists of darkening a few colors, as
				their projector is quite bright for a small room.
			*/
			.reveal {
				color: #bbb;
			}

			.hideable-foot {
				display: block;
				opacity: 0;
				font-size: 95% !important;
				position: absolute;
				bottom: 0em;
				right: 0.5em;
				text-align: right;
				transition: all 2s;
				z-index: 1;
			}

			.show-foot-pbt #foot-pbt,
			.show-foot-how #foot-how,
			.show-foot-rohl #foot-rohl,
			.show-foot-qhl #foot-qhl,
			.show-foot-qhl-noise #foot-qhl-noise,
			.show-foot-qbs #foot-qbs,
			.show-foot-lfpe #foot-lfpe,
			.show-foot-lw #foot-lw,
			.show-foot-smc #foot-smc,
			.show-foot-better #foot-better,
			.show-foot-title-foot #foot-title-foot,
			.show-foot-condensation #foot-condensation,
			.show-foot-qinfer #foot-qinfer,
			.show-foot-rfpe #foot-rfpe,
			.show-foot-thanks-ian #foot-thanks-ian,
			.show-foot-thanks-hpd #foot-thanks-hpd,
			.show-foot-fast-pe #foot-fast-pe,
			.show-foot-thesis #foot-thesis {
				opacity: 1 !important;
				transition: all 2s;
				z-index: 2;
			}

			.two-col-container {
				width: 100%;
			}

			.left-col {
				float: left;
				width: 70%;
			}

			.right-col {
				float: right;
				width: 30%;
			}
		</style>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!--
				This is an awful hack. Don't use it if you can help it.
				See https://www.raymondcamden.com/2014/04/01/Adding-an-Absolutely-Positioned-Header-to-Revealjs/ for why it "works".
			-->
			<p class="hideable-foot" id="foot-title-foot">
				<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br>
				<!-- <cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/tf3">tf3</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/7nx">7nx</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/bk9d">bk9d</a></cite><br> -->
				<a href="https://www.cgranade.com/research/talks/quics/07-2016">
					www.cgranade.com/research/talks/quics/07-2016
				</a>
			</p>

			<p class="hideable-foot" id="foot-how">
				Ferrie, Granade, Cory <cite class="doi"><a href="https://dx.doi.org/10/tfx">tfx</a></cite>
			</p>

			<p class="hideable-foot" id="foot-rohl">
				Granade <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qhl">
				Wiebe <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tf3">tf3</a></cite>
			</p>
			
			<p class="hideable-foot" id="foot-qbs">
				Wiebe <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/7nx">7nx</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qhl-noise">
				Wiebe <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tdk">tdk</a></cite>
			</p>

			<p class="hideable-foot" id="foot-pbt">
				Granade, Combes, Cory <cite class="doi"><a href="https://dx.doi.org/10/bhdw">bhdw</a></cite>
			</p>

			<p class="hideable-foot" id="foot-rfpe">
				Wiebe and Granade <cite class="doi"><a href="https://dx.doi.org/10/bk9d">bk9d</a></cite>
			</p>

			<p class="hideable-foot" id="foot-lw">
				Liu and West <cite class="doi"><a href="https://dx.doi.org/10/8c2">8c2</a></cite>
			</p>

			<p class="hideable-foot" id="foot-smc">
				Doucet <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/bmch">bmch</a></cite>
			</p>

			<p class="hideable-foot" id="foot-lfpe">
				Ferrie and Granade <cite class="doi"><a href="https://dx.doi.org/10/tdj">tdj</a></cite>
			</p>

			<p class="hideable-foot" id="foot-thesis">
				Granade
				<cite class="hdl"><a href="https://dx.doi.org/10012/9217"><span>10012/</span>9217</a></cite>
			</p>

			<p class="hideable-foot" id="foot-thanks-hpd">
				<em>figure</em>: Ferrie
				<cite class="doi"><a href="https://dx.doi.org/10/tb4">tb4</a></cite>
			</p>

			<p class="hideable-foot" id="foot-fast-pe">
				Svore <em>et al.</em>
				<cite class="arxiv"><a href="https://arxiv.org/abs/1304.0741">1304.0741</a></cite>
			</p>

<!-- 
			<p class="hideable-foot" id="foot-thanks-ian">
				Collaboration w/ Hincks, Wang, Mirkamali, and Moussa.
			</p>
 -->

			<!-- <p class="hideable-foot" id="foot-better">
				Sergeevich <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/c4vv95">c4vv95</a></cite>,
				Ferrie <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tfx">tfx</a></cite>, 
				Hall and Wiseman <cite class="doi"><a href="https://dx.doi.org/10/bh6v">bh6v</a></cite>
			</p> -->

			<p class="hideable-foot" id="foot-condensation">
				Isard and Blake <cite class="doi"><a href="https://dx.doi.org/10/cc76f6">cc76f6</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qinfer">
				<a href="http://qinfer.org">qinfer.org</a>
			</p>

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-state="show-foot-title-foot">
					<h1 style="font-size: 175%">Rejection and Particle Filtering for Hamiltonian Learning</h1>
					<hr>
					<p>
						<a href="https://www.cgranade.com">Cassandra E. Granade</a> <br>
						<a href="https://equs.org/">Centre for Engineered Quantum Systems</a>
                        $
							\newcommand{\ee}{\mathrm{e}}
							\newcommand{\ii}{\mathrm{i}}
							\newcommand{\dd}{\mathrm{d}}
							\newcommand{\id}{&#x1d7d9;}
							\newcommand{\TT}{\mathrm{T}}
							\newcommand{\defeq}{\mathrel{:=}}
							\newcommand{\Tr}{\operatorname{Tr}}
							\newcommand{\Var}{\operatorname{Var}}
							\newcommand{\Cov}{\operatorname{Cov}}
							\newcommand{\rank}{\operatorname{rank}}
							\newcommand{\expect}{\mathbb{E}}
							\newcommand{\sket}[1]{|#1\rangle\negthinspace\rangle}
							\newcommand{\sbraket}[1]{\langle\negthinspace\langle#1\rangle\negthinspace\rangle}
							\newcommand{\Gini}{\operatorname{Ginibre}}
							\newcommand{\supp}{\operatorname{supp}}
							\newcommand{\ket}[1]{\left|#1\right\rangle}
							\newcommand{\bra}[1]{\left\langle#1\right|}
							\newcommand{\braket}[1]{\left\langle#1\right\rangle}
						$
					</p>
					
					<!-- <p>
						joint work with<br>
						Nathan Wiebe • Christopher Ferrie • D. G. Cory
					</p> -->
				</section>

				<section>
					<p>
						Learning Hamiltonians is critical to a range of QIP tasks:
					</p>

					<dl>
						<dt>Metrology</dt>
						<dd>Learning magnetic fields, etc.</dd>

						<dt>Calibration</dt>
						<dd>Static field / pulse power / crosstalk, etc.</dd>

						<dt>Debugging/Diagnosis</dt>
						<dd>$T_2$ estimation, other noise finding</dd>

						<dt>Verification/Validation</dt>
						<dd>Analog and digital quantum simulation</dd>
					</dl>
				</section>

				<section data-markdown>
					### **Example**: Ramsey Estimation ###

					Suppose $H = \omega \sigma_z / 2$ for some unknown $\omega$.

					Traditional approach:

					- Prepare $\ket{+} \propto \ket{0} + \ket{1}$, measure “click” w/ pr.:
					$
						\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2 = \cos^2(\omega t / 2)
					$.
					- Repeat for many “shots” to estimate click pr.
					- Repeat for many times to estimate signal.
				</section>

				<section>
					You'll get something that looks a bit like this:

					<img src="figures/rabi-example-signal.png" width="90%">
				</section>

				<section>
					What's $\omega$? Fourier transform and look at the peak.

					<img src="figures/rabi-example-spectrum.png" width="90%">
				</section>

				<section>
					<div data-markdown>
						We can do better.

						# $H = H(\vec{x})$. # 

						Hamiltonian learning is a special case of *parameter estimation*:
						given data $D$, what is $\vec{x}$?
					</div>

					<div class="fragment" data-markdown>
						---

						We want an approach that can work for small and large quantum devices
						alike.
					</div>
				</section>

				<section>
					<p>
						To get there, we consider several different approaches to
						parameter estimation.
					</p>

					<dl style="font-size: 85%" class="border-dl">
						<dt>
							Analytic
							<span class="right-note">
								Sergeevich <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/c4vv95">c4vv95</a></cite>
							</span><br>&nbsp;
							<span class="right-note">
								Ferrie, Granade, and Cory <cite class="doi"><a href="https://dx.doi.org/10/tfx">tfx</a></cite>
							</span>
						</dt>
						<dt>
							Rejection filter
							<span class="right-note">
								Wiebe and Granade <cite class="doi"><a href="https://dx.doi.org/10/bk9d">bk9d</a></cite>
							</span>
						</dt>
						<dt>
							Particle filter
							<span class="right-note">
								Doucet <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/bmch">bmch</a></cite>
							</span>
						</dt>
						<dt>
							Likelihood-free particle filter
							<span class="right-note">
								Ferrie and Granade <cite class="doi"><a href="https://dx.doi.org/10/tdj">tdj</a></cite>
							</span>
						</dt>
					</dl>
				</section>

				<section data-markdown>
					### The Likelihood Function ###

					$\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2$
					  defines probability $\Pr(d | \omega; t)$ for every
					  outcome $d$, model $\omega$ and experiment $t$.

					Basis for both maximum-likelihood and Bayesian methods.
				</section>

				<section data-markdown>
					### Bayesian Parameter Estimation ###

					The likelihood tells us what we learn from data:

					$$
						\Pr(\vec{x} | d; e) = \frac{\Pr(d | \vec{x}; e)}{\Pr(d | e)} \Pr(\vec{x})
					$$

					---

					Estimate $\hat{x} = \expect[\vec{x} | d; e] = \int \vec{x} \Pr(\vec{x} | d; e)\dd \vec{x}$.

					- **Optimal** for mean&mdash;squared error.
				</section>

				<section data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/bayesian-pe-flowchart.png">
					
				</section>

				<section data-markdown>
					Each posterior $\Pr(\vec{x} | d; e)$ encodes our uncertainty about
					$x$. What should we expect squared-error to be?

					**Scalar case**:
					$
						\expect[(x - \hat{x})^2 | d; e] = 
                        \mathbb{V}[x \mid d; e].
					$

					**Vector case**: $\expect[(\vec{x} - \hat{x}) (\vec{x} - \hat{x})^\TT | d; e]
					= \Cov(\vec{x} | d; e)$.
				</section>

				<section data-markdown>
					We can use our posteriors to make *adaptive* decisions:
					$$
						e_* = \operatorname{arg min}_e \expect_d[\mathbb{V}(x | d; e)]
					$$
				</section>

				<section data-markdown data-state="show-foot-how">
					### **Example**: $x = (\omega)$ ###

					Can analytically find posterior for Gaussian priors,
					use to adaptively choose $t_k$.

					![](./figures/how-t2.png)
				</section>

				<section>
					<p data-markdown>
						**Problem**: may be intractable to analytically compute $$
							\hat{x} \defeq
							\int \Pr(\vec{x} | d; e) \dd\vec{x} =
							\int \frac{
								\Pr(d | \vec{x}; e)
							}{
								\int \Pr(d | \vec{x}; e) \Pr(\vec{x}) \dd\vec{x}
							} \Pr(\vec{x}) \dd\vec{x}.
						$$
					</p>

					<div class="fragment" data-markdown>
						---

						**Answer**: numerically approximate $\int f(\vec{x}) \Pr(\vec{x} | d)\dd\vec{x}$.

						*Efficient* to use Monte Carlo integration if we can sample from $\Pr(\vec{x})$.
					</div>
				</section>
<!-- 
				<section data-markdown>
					## Monte Carlo Integration ##

					$$
						\int f(\vec{x}) p(\vec{x})\dd\vec{x} \approx \frac{1}{N} \sum_i f(\vec{x}_i)
						\text{ for } \left\\{\vec{x}_i\right\\} \sim p(\vec{x})
					$$

					---

					Efficient *if* we can sample from $p(\vec{x})$.

					How do we do that for $p(\vec{x}) = \Pr(\vec{x} | d; e)$?
				</section> -->

				<section data-markdown>
					## Rejection Sampling ##

					Given samples from $\Pr(\vec{x})$ and likelihood
					function $\Pr(d | \vec{x}; e)$, how do we sample
					from posterior for datum $d$?

					- Draw $\vec{x} \sim \Pr(\vec{x})$, 
					  accept $\vec{x}$ w/ $\Pr(d | \vec{x}; e)$.

					Accepted samples are distributed according to posterior.
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="80%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/rejs-example.png"
						 >
				</section>

				<section>
					<h3>Rejection Sampling Isn't Enough</h3>

					<p>
						Let $D = {d_1, \dots, d_k}$ be a set of data.
					</p>
					<p>
						$$
							\Pr(\text{accept} | \vec{x}) = \Pr(D | \vec{x}) = \prod_{d \in D} \Pr(d | \vec{x})
							\overset{k \to \infty}{\longrightarrow} 0.
						$$
					</p>

					<hr>

					<h4><strong>Example:</strong> Biased Coin $x = (p)$</h4>
					<p>
						$\Pr(H | p) = p$, $d \in \{H, T\}$.
					</p>

					<p>
						$p \approx 0.5 \Longrightarrow \Pr(d_1, \dots, d_k | p) \approx 1 / 2^k$.
					</p>

					<p>
						We will accept exponentially few samples!
					</p>
				</section>

				<section>
					<div data-markdown>
						### Gaussian Resampling ###

						For each datum $d$, use rejection sampling to approximate posterior
						moments:

						- $\bar{x} \gets \expect[\vec{x} | d]$.
						- $\Sigma \gets \operatorname{Cov}[\vec{x} | d] = \expect[\vec{x} \vec{x}^\TT | d] - \bar{x} \bar{x}^\TT$.
					</div>

					<div data-markdown class="fragment">
						---

						At the next iteration, draw prior samples from Gaussian with these moments:
						$$
							\vec{x} \mid d \sim \mathcal{N}(\bar{x}, \Sigma)
						$$

						Keeps $\Pr(\text{accept}) \approx \text{constant}$.
					</div>
				</section>

				<section>
					<p>
						Can compute $\bar{x}$, $\Sigma$ from
						one sample at a time by accumulating
					</p>
					<p>
						$$
							x_{\Sigma} = \sum x
							\text{ and }
							x^2_{\Sigma} = \sum x^2.
						$$
					</p>

					<p>
						\begin{align}
							\bar{x} &amp; = x_{\Sigma} / n_{\text{accept}} \\
							\Sigma &amp; = x^2_{\Sigma} / n_{\text{accept}} - \bar{x}^2.
						\end{align}
					</p>

					<p>
						<em>Welford's algorithm</em>: numerically-stable modification.
					</p>
				</section>

				<section data-state="show-foot-rfpe">
					<h2>Rejection Filtering (RejF)</h2>

					<p>
					    <strong>Input:</strong>
					    Prior mean $\bar{x}$, prior covariance $\Sigma$,
						number of attempts $m$.
					<p>

					<ul style="width: 80%">
						<li><strong>For</strong> each datum $d$ and experiment $e$:</li>
						<ul>
							<li>
								$n, \bar{x}', M_2 \gets 0$
								<span class="comment">Initialize Welford.</span>
							</li>
							<li><strong>While</strong> $n < m$:</li>
							<ul>
								<li>
									Draw $\vec{x} \sim \mathcal{N}(\bar{x}, \Sigma)$.
									<span class="comment">Sample f/ prior.</span>
								</li>
								<li>Accept $\vec{x}$ w/ $\Pr(d | \vec{x}; e)$.</li>
								<li><strong>If</strong> accepted, update $n$, $\bar{x}'$, $M_2$.</li>
							</ul>
							<li>
								$\bar{x} \gets \bar{x}'$, $\Sigma \gets M_2 / (n - 1)$.
								<span class="comment">
									Est. moments.
								</span>
							</li>
						</ul>
					</ul>
				</section>

				<section data-markdown>
					## Advantages of RejF ##

					- Accept pr based on a single datum
					  ⇒ likelihood $\not\to 0$.
					- Easy to implement
					- Never needed to remember each accepted $x$!
						- Very low-memory (constant # of
						  registers), ideal for FPGA use.
					- Easily parallelizable
					- “Reset rule” can abort from approximation failures.
				</section>

				<section data-state="show-foot-fast-pe">
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<p>
						Prepare state $\ket{\phi}$ s. t. $U\ket{\phi} = \ee^{\ii \phi}\ket{\phi}$,
						measure to learn $\phi$.
					</p>

					<img src="./figures/pe-circuit-crop.png">

					<p>
						$\theta = 0 \Rightarrow$ freq. est likelihood, w/ $\phi = \omega$, $M = t$.
					</p>

					<dl style="font-size: 65%">
						<dd style="position: relative; left: -1.7em; top: 0.75em"><h4>
							Applications
						</h4></dd>

						<dt>Interferometry / metrology
							<span class="right-note">
								Higgins <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/crwd6w">crwd6w</a></cite>
							</span>
						</dt>

						<dt>Gate calibration / robust PE
							<span class="right-note">
								Kimmel <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/bmrg">bmrg</a></cite>
							</span>
						</dt>

						<dt>Quantum simulation and chemistry
							<span class="right-note">
								Reiher <em>et al.</em>
								<cite class="arxiv"><a href="https://arxiv.org/abs/1605.03590">1605.03590
								</a></cite>
							</span>
						</dt>
					</dl>
				</section>


				<section data-state="show-foot-rfpe">
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<img src="./figures/pe-error.png" width="80%">
				</section>

				<section>
					<p  data-markdown>
						**Drawback**: RejF requires posterior after each datum
						to be $\approx$ Gaussian.
					</p>

					<p class="fragment" data-markdown>
						We can solve this by using a more general approach:

						- Weaken Gaussian assumption.
						- Generalize the rejection sampling step.
					</p>
				</section>

				<section data-state="show-foot-lw">
					<h2>Liu-West Resampler</h2>

					<p>
						If we remember each sample $\vec{x}$, we can use them to
						relax RejF assumptions.
					</p>

					<p>
						<strong>Input:</strong>
						$a, h \in [0, 1]$ s.t. $a^2 + h^2 = 1$,
						distribution $p(\vec{x})$.
					</p>

					<ul>
						<li>
							Approximate $\bar{x} \gets \expect[\vec{x}]$, $\Sigma \gets \operatorname{Cov}(\vec{x})$
						</li>
						<li>Draw <em>parent</em> $\vec{x}$ from $p(\vec{x})$.</li>
						<li>Draw $\vec{\epsilon} \sim \mathcal{N}(0, \Sigma)$.</li>
						<li>
							<strong>Return</strong>
							new sample $\vec{x}' \gets a \vec{x} + (1 - a) \bar{x} + h \vec{\epsilon}$.
						</li>
					</ul>
				</section>

				<section data-markdown>
					### Why Does Liu-West Work? ###

					\begin{align}
						\vec{x}'          &amp; \gets a \vec{x} + (1 - a) \bar{x} + h \vec{\epsilon} \\\\
						\expect[\vec{x}'] &amp; = [a + (1 - a)] \bar{x} \\\\
						\Cov(\vec{x}')    &amp; = (a^2 + h^2) \Cov(\vec{x}). \\\\
						\Longrightarrow a^2 + h^2 &amp; = 1 \text{ preserves } \expect[\vec{x}], \Cov(\vec{x}).
					\end{align}

					---

					Mixes original approximation with $1 - a$ of a Gaussian
					matching moments.

					- $a \to 0$: RejF (assumed density) approx
					- $a \to 1$: Bootstrap
					- $a = 0.98$: typical case (2% Gaussian).
				</section>

				<section data-markdown>
					### From Samples to Particles ###

					Define a particle $(w_i, \vec{x}_i)$ as
					a sample $\vec{x}_i$ and a weight $w_i \in [0, 1]$.

					- $\expect[\vec{x}] = \sum_i w_i \vec{x}_i$
					- $\Cov(\vec{x}) =
					  \sum_i w_i \vec{x}_i \vec{x}_i^\TT - \expect[\vec{x}]\expect^\TT[\vec{x}]$

					- $\expect[f(\vec{x})] = \sum_i w_i f(\vec{x}_i)$

					---

					Corresponds to
					$
						p(\vec{x}) \approx \sum_i w_i \delta(\vec{x} - \vec{x}_i).
					$
				</section>

				<section>
					<p>
						Particles can represent distributions using either<br>
						<span style="color: #D55E00;">weights</span> or
						<span style="color: #56B4E9;">positions</span>.
					</p>

					<img src="figures/impovrishment.png" class="stretch">
				</section>


				<section data-state="show-foot-smc">
					<h2>Particle Filter</h2>

					<ul>
						<li>
							Draw $N$ initial samples $\vec{x}_i$ from the prior $\Pr(\vec{x})$
							w/ uniform weights.
						</li>

						<li>
							Instead of rej. sampling, update <span style="color: #D55E00;">weights</span>
							by
							\begin{align}
								\tilde{w}_i &amp; = w_i \times \Pr(d | \vec{x}_i; e)
							\end{align}
						</li>
						<li>
							Renormalize.
							\begin{align}
								w_i &amp; \mapsto \tilde{w}_i / \sum_i \tilde{w}_i
							\end{align}
						</li>

						<li>
							Periodically use Liu-West to draw new $\{\vec{x}_i\}$
					  		with uniform weights.
					  		<span class="comment">
					  			Store posterior in <span style="color: #56B4E9;">positions</span>.
					  		</span>
					  	</li>
					</ul>
				</section>

				<section data-background="#000" data-background-video="figures/multicos-distributions.mp4">					
				</section>


				<section>
					<h4>Useful for Hamiltonian models...</h4>

					<dl style="font-size: 80%" class="border-dl">
						<dt>Rabi/Ramsey/Phase est.
							<span class="right-note">
								Granade <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>
							</span class="right-note">
						</dt>

						<dt>Swap spectroscopy
							<span class="right-note">
								Stenberg <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/7nw">7nw</a></cite>
							</span>
						</dt>
					</dl>

					<h4 style="padding-top: 0.5em;">
						...as well as other QIP tasks.
					</h4>

					<dl style="font-size: 80%" class="border-dl">
						<dt>Tomography
							<span class="right-note">
								Huszár and Holsby
								<cite class="doi"><a href="https://dx.doi.org/10/s86">s86</a></cite>
							</span><br>&nbsp;
							<span class="right-note">					
								Struchalin <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/bmg5">bmg5</a></cite>
							</span><br>&nbsp;
							<span class="right-note">					
								Ferrie
								<cite class="doi"><a href="https://dx.doi.org/10/7nt">7nt</a></cite>							
							</span><br>&nbsp;
							<span class="right-note">
								Granade <em>et al.</em>						
								<cite class="doi"><a href="https://dx.doi.org/10/bhdw">bhdw</a></cite>,
								<cite class="arxiv"><a href="https://arxiv.org/abs/1605.05039">1605.05039</a></cite>
							</span>
						</dt>

						<dt>Randomized benchmarking
							<span class="right-note">
								Granade <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/zmz">zmz</a></cite>
							</span>
						</dt>

						<dt>Continuous measurement
							<span class="right-note">
								Chase and Geremia
								<cite class="doi"><a href="https://dx.doi.org/10/chk4q7">chk4q7</a></cite>
							</span>
						</dt>

						<dt>Interferometry/metrology
							<span class="right-note">
								Granade
								<cite class="hdl"><a href="https://dx.doi.org/10012/9217"><span>10012/</span>9217</a></cite>
							</span>
						</dt>
					</dl>
				</section>


				<section data-markdown data-state="show-foot-rohl">
					### **Example**: $\vec{x} = (\omega, T_2)$ ###

					![](figures/unknown-t2.png)
				</section>

				<section data-state="show-foot-thesis">
					<h3><strong>Example</strong>: $\vec{x} =$ COSY / Crosstalk</h3>

					<p>\(
						H(\omega_1, \omega_2, J) = \frac12 \left(
							\omega_1 \sigma_z \otimes \id + 
							\omega_2 \id \otimes \sigma_z +
							J \sigma_z \otimes \sigma_z
						\right)
					\)</p>

					<img src="figures/cosy-pulses.png">
				</section>

				<section data-state="show-foot-thesis">
					<h3><strong>Example</strong>: $\vec{x} =$ COSY / Crosstalk</h3>

					<img src="figures/cosy-loss.png" width="80%">
				</section>

				<!-- <section data-state="show-foot-thanks-ian"
						 data-background="figures/estimator-sim.png"
						 data-background-size="80%"
						 data-background-transition="slide">

				    <h3 
						 style="position: relative; top: -3.5em;"
					><strong>Example</strong>: $\vec{x} =$ NV Hamiltonian</h3>

					<p style="position: relative; top: 6em;">
						Estimated Hamiltonian reproduces experimental
						data.
					</p>

				</section> -->

				<section data-state="show-foot-thanks-hpd">
					<h3>Error Bars</h3>

					<p>
						Particle filters also yield <em>credible regions</em> $X_\alpha$ s.t.
						$$
							\Pr(\vec{x} \in X_\alpha | d; e) \ge \alpha.
						$$
					</p>

					<img src="figures/nc.png" class="stretch">

					<p>
						E.g.: Posterior covariance ellipse, convex hull, MVEE.
					</p>

				</section>

				<section data-markdown data-state="show-foot-qinfer">
					#### **QInfer**: Particle Filter Implementation for Quantum Info ####

					We provide an open-source implementation for use in experiment and theory alike.

					Written in Python, works with MATLAB and Julia.
				</section>

				<section>
					<h3>Assessing Performance</h3>

					<dl>
						<dt>
							Risk: <span class="right-note">
								average error <strong>given</strong> $\color{white}{\vec{x}}$
							</span>
						</dt>

						<dt>
							Bayes risk: <span class="right-note">
								average error <strong>over</strong> $\color{white}{\vec{x}}$
							</span>
						</dt>
					</dl>

					<div data-markdown class="fragment">
						---

						- Exact $\hat{x}$ optimal for Bayes risk / squared error.
						- Difficult to *prove* particle filter risk; find in special
						  cases or by running many trials.
					</div>
				</section>

				<section>
					<h3>Simulation Costs</h3>

					<p style="font-size: 140%">
						\begin{align}
							\tilde{w}_i &amp; = w_i \times \color{red}{\Pr(d | \vec{x}_i; e)} \\
							w_i &amp; \mapsto \tilde{w}_i / \sum_i \tilde{w}_i
						\end{align}
					</p>

					<div data-markdown>
						- $\Pr(d | \vec{x}_i; e)$ is a *simulation*.
						- Need to
						  simulate for each particle (~1000s calls/datum).
						- Infeasible for large quantum models.

						Let's do better: use *quantum* simulation instead.
					</div>
				</section>

				<section data-markdown>
					### Two Kinds of Simulation ###

					![](figures/simulators.png)

					Weak simulators produce *plausible datasets*.
				</section>

				<section data-markdown>
					### Likelihood-Free RejF ###

					Replace rej. sampling step by drawing
					datum from likelihood instead of computing
					exact value:

					- Draw datum $d'$ from $\Pr(d | \vec{x}; e)$.
					- Accept $\vec{x}$ if $d = d'$.
				</section>

				<section data-markdown data-state="show-foot-lfpe">
					### Likelihood-Free Particle Filtering ###

					Can also use frequencies $f$ from weak simulation to
					approximate likelihoods
					in particle filtering.

					$$
						\widehat{\Pr}(d | \vec{x}; e) =
							\frac{f(d | \vec{x}; e)}{k}
					$$

					$k$: number of weak simulation calls used.
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak-desaturated.png"
						 >
					#### **Example**: Noisy Coin ####

					How well can we learn the bias $x = (p)$ of a noisy coin?

					$$
						\Pr(\text{click} | p) = 0.95 p + 0.1 (1 - p)
					$$
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak.png"
						 data-background-transition="fade-in"
						 data-transition="fade-in">
				</section>

				<section data-markdown data-state="show-foot-qhl">
					### Quantum Hamiltonian Learning ###

					![](figures/qle.png)
				</section>

				<section data-markdown>
					### **Example**: Nearest-Neighbor 1D Ising ###

					![](figures/qle-line-ising.png)
				</section>

				<section data-markdown>
					We can do more with access to a *trusted*
					simulator.


					### Quantum Interactivity ### 
					
					![](figures/iqle.png)

					Perform weak simulations on trusted device only.
				</section>

				<section>
					<p>
						We design experiments using the
					</p>

					<h3><strong>PGH</strong>: Particle Guess Heuristic</h3>

					<ul>
						<li>
							Draw $\vec{x}_-, \vec{x}_-'$ from current
					  		posterior.
					  	</li>
						<li>Let $t = 1 / |\vec{x}_- - \vec{x}_-'|$.</li>
						<li><strong>Return</strong> $e = (\vec{x}_-, t)$.</li>
					</ul>

					<div class="fragment">
						<hr>

						<p>
							Adaptively chooses experiments such that<br>
							$t |\vec{x}_- - \vec{x}_-'| \approx\,$ constant.
						</p>
					</div>
				</section>

				<section data-markdown>
					### **Example**: Ising on Complete Graph ###

					![](figures/iqle-complete-ising.png)
				</section>

				<section data-markdown data-state="show-foot-qhl-noise">
					Robust even to wrong model. ($0.5$ NN + $10^{-4}$ Complete)

					![](figures/qhl-bad-model.png)
				</section>

				<section data-state="show-foot-qbs">
					<p>
						One important approximation: physical locality.
					</p>

					<img src="./figures/qbs-lightcones.png" width="65%">

					<p>
						Approximation quality can be bounded if
						Lieb-Robinson velocity is finite.
					</p>
				</section>

				<section>
					<p>
						Scan trusted device across untrusted.
					</p>

					<img src="./figures/scanning.png" width="55%">

					<p>
						Run particle filter <em>only</em> on supported
						parameters.
					</p>
				</section>

				<section>
					<h4>50 qubit Ising chain, 8 qubit simulator, 4 qubit observable</h4>

					<img src="./figures/qbs-error-per-scan.png" width="60%">
				</section>

				<section>
					<h3>Going Further</h3>

					<dl class="border-dl">
						<dt>Hyperparameterization
							<span class="right-note">
								Granade <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>
							</span>
						</dt>
						<dd>
							$\Pr(d | y) = \expect_x[\Pr(d | x) \Pr(x | y)]$.<br>
							Allows composing w/ noise, inhomogeneity, etc.
						</dd>

						<dt>Time-dependence
							<span class="right-note">
								Isard and Blake
								<cite class="doi"><a href="https://dx.doi.org/10/cc76f6">cc76f6</a></cite>
							</span>
						</dt>
						<dd>
							Adding timestep update allows learning stochastic
							processes.
						</dd>

						<dt>Model selection							
							<span class="right-note">
								Ferrie
								<cite class="doi"><a href="https://dx.doi.org/10/7nt">7nt</a></cite>
							</span>
						</dt>
						<dd>
							Using acceptance ratio or normalizations
							enables comparing models.
						</dd>
					
						<dt>Quantum filtering
							<span class="right-note">
								Wiebe and Granade
								<cite class="arxiv"><a href="http://arxiv.org/abs/1512.03145">1512.03145</a></cite>
							</span>
						</dt>
						<dd>
							Rejection filtering can be quantized using
							Harrow <em>et al.</em>
							<cite class="doi"><a href="https://dx.doi.org/10/bcz3hc">bcz3hc</a></cite>.
						</dd>
					</dl>
				</section>

				<section>
					<h1 class="thanks">Thank you!</h1>
				</section>

				<section>
					<h2>Welford's Algorithm</h2>
					
					<p>
						Can compute $\bar{x}$, $\Sigma$ from
						one sample at a time. Numerically stable.
					</p>

					<ul style="width: 80%">
						<li>$n, \bar{x}, M_2 \gets 0$.</li>
						<li><strong>For</strong> each sample $x$:</li>
						<ul>
							<li>
								$n \gets n + 1$
								<span class="comment">Record # of samples</span>
							</li>
							<li>
								$\Delta \gets x - \mu$
								<span class="comment">Diff to running mean</span>
							</li>
							<li>
								$\bar{x} \gets \bar{x} + \Delta / n$
								<span class="comment">Update running mean</span>
							</li>
							<li>
								$M_2 \gets M_2 + \Delta (x - \bar{x})$
								<span class="comment">Update running var</span>
							</li>
						</ul>
						<li><strong>Return</strong> mean $\bar{x}$, variance $M_2 / (n - 1)$.</li>
					</ul>

					<p>
						Vector case is similar.
					</p>
				</section>



			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: true,
				zoomKey: 'shift',

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/0.9.16/socket.io.min.js', async: true },
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
