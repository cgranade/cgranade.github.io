<!doctype html>
<html lang="en">

<!--
	ABSTRACT
	========

	Many tasks in quantum information rely on accurate knowledge of a system's Hamiltonian, including calibrating control, characterizing devices, and verifying quantum simulators. In this talk, we pose the problem of learning Hamiltonians as an instance of parameter estimation. We then solve this problem with Bayesian inference, and describe how rejection and particle filtering provide efficient numerical algorithms for learning Hamiltonians. Finally, we discuss how filtering can be combined with quantum resources to verify quantum systems beyond the reach of classical simulators.
-->

	<head>
		<meta charset="utf-8">

		<title>Rejection and Particle Filtering for Quantum Information</title>

		<meta name="author" content="Christopher E. Granade">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">

 		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">
		<link rel="stylesheet" href="css/theme/cgranade-dark.css" id="theme">


		<style type="text/css" media="screen">
			h1.thanks {
				font-size: 330%;
			}

			.hideable-foot {
				display: block;
				opacity: 0;
				font-size: 90% !important;
				position: absolute;
				bottom: 0em;
				right: 0.5em;
				text-align: right;
				transition: all 2s;
				z-index: 1;
			}

			.show-foot-pbt #foot-pbt,
			.show-foot-how #foot-how,
			.show-foot-rohl #foot-rohl,
			.show-foot-qhl #foot-qhl,
			.show-foot-qhl-noise #foot-qhl-noise,
			.show-foot-qbs #foot-qbs,
			.show-foot-lfpe #foot-lfpe,
			.show-foot-lw #foot-lw,
			.show-foot-smc #foot-smc,
			.show-foot-better #foot-better,
			.show-foot-title-foot #foot-title-foot,
			.show-foot-condensation #foot-condensation,
			.show-foot-qinfer #foot-qinfer,
			.show-foot-rfpe #foot-rfpe,
			.show-foot-thanks-ian #foot-thanks-ian,
			.show-foot-thanks-hpd #foot-thanks-hpd,
			.show-foot-thanks-bayes-neon #foot-thanks-bayes-neon,
			.show-foot-thanks-statistics-neon #foot-thanks-statistics-neon,
			.show-foot-fast-pe #foot-fast-pe,
			.show-foot-thesis #foot-thesis {
				opacity: 1 !important;
				transition: all 2s;
				z-index: 2;
			}

			.two-col-container {
				width: 100%;
			}

			.left-col {
				float: left;
				width: 70%;
			}

			.right-col {
				float: right;
				width: 30%;
			}
		</style>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!--
				This is an awful hack. Don't use it if you can help it.
				See https://www.raymondcamden.com/2014/04/01/Adding-an-Absolutely-Positioned-Header-to-Revealjs/ for why it "works".
			-->
			<p class="hideable-foot" id="foot-title-foot">
				<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br>
				<!-- <cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/tf3">tf3</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/7nx">7nx</a></cite>,
				<cite class="doi"><a href="https://dx.doi.org/10/bk9d">bk9d</a></cite><br> -->
				<a href="http://www.cgranade.com/research/talks/qml/2016">
					www.cgranade.com/research/talks/qml/2016
				</a> • <a href="http://nbviewer.jupyter.org/github/cgranade/cgranade.github.io/blob/master/research/talks/qml/2016/qinfer-tutorial.ipynb">
					tutorial
				</a>
			</p>

			<p class="hideable-foot" id="foot-how">
				Ferrie, Granade, Cory <cite class="doi"><a href="https://dx.doi.org/10/tfx">tfx</a></cite>
			</p>

			<p class="hideable-foot" id="foot-rohl">
				Granade <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qhl">
				Wiebe <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tf3">tf3</a></cite>
			</p>
			
			<p class="hideable-foot" id="foot-qbs">
				Wiebe <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/7nx">7nx</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qhl-noise">
				Wiebe <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tdk">tdk</a></cite>
			</p>

			<p class="hideable-foot" id="foot-pbt">
				Granade, Combes, Cory <cite class="doi"><a href="https://dx.doi.org/10/bhdw">bhdw</a></cite>
			</p>

			<p class="hideable-foot" id="foot-rfpe">
				Wiebe and Granade <cite class="doi"><a href="https://dx.doi.org/10/bk9d">bk9d</a></cite>
			</p>

			<p class="hideable-foot" id="foot-lw">
				Liu and West <cite class="doi"><a href="https://dx.doi.org/10/8c2">8c2</a></cite>
			</p>

			<p class="hideable-foot" id="foot-smc">
				Doucet <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/bmch">bmch</a></cite>
			</p>

			<p class="hideable-foot" id="foot-lfpe">
				Ferrie and Granade <cite class="doi"><a href="https://dx.doi.org/10/tdj">tdj</a></cite>
			</p>

			<p class="hideable-foot" id="foot-thesis">
				Granade
				<cite class="hdl"><a href="https://dx.doi.org/10012/9217"><span>10012/</span>9217</a></cite>
			</p>

			<p class="hideable-foot" id="foot-thanks-hpd">
				<em>figure</em>: Ferrie
				<cite class="doi"><a href="https://dx.doi.org/10/tb4">tb4</a></cite>
			</p>

			<p class="hideable-foot" id="foot-fast-pe">
				Svore <em>et al.</em>
				<cite class="arxiv"><a href="https://arxiv.org/abs/1304.0741">1304.0741</a></cite>
			</p>

			<p class="hideable-foot" id="foot-thanks-statistics-neon">
				<!--
					A note about the word "neon": this is not technically correct in
					the chemical sense, but is the word that the artist chose to describe
					their work, and so I choose it here out of deference to that description.
				-->
				background: neon by
				<a href="http://oresttataryn.com/">Orest Tataryn</a>
			</p>

			<p class="hideable-foot" id="foot-thanks-bayes-neon">
				background: photo by <a href="https://commons.wikimedia.org/wiki/File:Bayes%27_Theorem_MMB_01.jpg">
					mattbuck
				</a>,
				h/t Wiebe
			</p>

<!-- 
			<p class="hideable-foot" id="foot-thanks-ian">
				Collaboration w/ Hincks, Wang, Mirkamali, and Moussa.
			</p>
 -->

			<!-- <p class="hideable-foot" id="foot-better">
				Sergeevich <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/c4vv95">c4vv95</a></cite>,
				Ferrie <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/tfx">tfx</a></cite>, 
				Hall and Wiseman <cite class="doi"><a href="https://dx.doi.org/10/bh6v">bh6v</a></cite>
			</p> -->

			<p class="hideable-foot" id="foot-condensation">
				Isard and Blake <cite class="doi"><a href="https://dx.doi.org/10/cc76f6">cc76f6</a></cite>
			</p>

			<p class="hideable-foot" id="foot-qinfer">
				<a href="http://qinfer.org">qinfer.org</a>
			</p>

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-state="show-foot-title-foot">
					<h1 style="font-size: 175%">Rejection and Particle Filtering for Hamiltonian Learning</h1>
					<hr>
					<p>
						<a href="http://www.cgranade.com">Christopher E. Granade</a> <br>
					</p>
					<p style="font-size: 80%; position: relative; top: -0.6em;">
						<a href="https://equs.org/">Centre for Engineered Quantum Systems</a><br>
						<a href="https://sydney.edu.au/">University of Sydney</a>
					</p>
					
					<!-- <p>
						joint work with<br>
						Nathan Wiebe • Christopher Ferrie • D. G. Cory
					</p> -->
				</section>

				<section>
					<h2>What is a Hamiltonian?</h2>

					<p>
						Lots of meanings and applications in condensed matter, quantum info, etc.
						In this talk, we consider precisely one:
					</p>

					<div class="fragment">
						<p style="font-size: 180%">$$
							\ket{\psi(t)} = \ee^{\ii H t} \ket{\psi(0)}
						$$</p>

						<p>
							A <span style="color: white;">Hamiltonian</span> <em>generates dynamics</em>.
						</p>
					</div>
				</section>

				<section>
					<p>
						Learning Hamiltonians is critical to a range of tasks:
					</p>

					<dl>
						<dt>Metrology</dt>
						<dd>Learning magnetic fields, etc.</dd>

						<dt>Calibration</dt>
						<dd>Static field / pulse power / crosstalk, etc.</dd>

						<dt>Debugging/Diagnosis</dt>
						<dd>$T_2$ estimation, other noise finding</dd>

						<dt>Verification/Validation</dt>
						<dd>Analog and digital quantum simulation</dd>
					</dl>
				</section>

				<section data-markdown>
					### **Example**: Ramsey Estimation ###

					Suppose $H = \omega \sigma_z / 2$ for some unknown $\omega$.

					Traditional approach:

					- Prepare $\ket{+} \propto \ket{0} + \ket{1}$, measure “click” w/ pr.:
					$
						\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2 = \cos^2(\omega t / 2)
					$.
					- Repeat for many “shots” to estimate click pr.
					- Repeat for many times to estimate signal.
				</section>

				<section>
					You'll get something that looks a bit like this:

					<img src="figures/rabi-example-signal.png" class="stretch">
				</section>

				<section>
					What's $\omega$? Fourier transform and look at the peak.

					<img src="figures/rabi-example-spectrum.png" width="90%">
				</section>

				<section
					data-background-image="figures/statistics-neon.jpeg"
					data-background-size="112%"
					data-state="show-foot-thanks-statistics-neon"
				>
					<div data-markdown style="position: relative; top: -1em;">
						We can do better.
					</div>

					<div class="fragment" data-markdown  style="position: relative; bottom: -1em;">
						# $H = H(\vec{x})$. # 

						Hamiltonian learning is a special case of *parameter estimation*:
						given data $D$, what is $\vec{x}$?
					</div>
				</section>

				<section>
					<p>
						We want an approach that can work for small and large quantum devices
						alike.
					</p>

					<div class="fragment">
						<hr>

						<p>
							<strong>Punchline:</strong> our algorithm can
							characterize 50-qubit devices.
						</p>
						<p>
							\begin{align}
								H &amp; = \sum_{i = 1}^{50} \sum_{j = 1}^{50} \omega_{ij}
									\sigma_z^{(i)} \sigma_z^{(j)}, \\
								\omega_{i,j} &amp; \sim \operatorname{Uniform}\left(0, 10^{-2 (|i - j| - 1)}\right).
							\end{align}
						</p>
					</div>
				</section>

				<section>
					<p>
						To get there, we consider several different approaches to
						parameter estimation.
					</p>

					<dl style="font-size: 85%" class="border-dl">
						<dt>
							Analytic
							<span class="right-note">
								Sergeevich <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/c4vv95">c4vv95</a></cite>
							</span><br>&nbsp;
							<span class="right-note">
								Ferrie, Granade, and Cory <cite class="doi"><a href="https://dx.doi.org/10/tfx">tfx</a></cite>
							</span>
						</dt>
						<dt>
							Rejection filter
							<span class="right-note">
								Wiebe and Granade <cite class="doi"><a href="https://dx.doi.org/10/bk9d">bk9d</a></cite>
							</span>
						</dt>
						<dt>
							Particle filter
							<span class="right-note">
								Doucet <em>et al.</em> <cite class="doi"><a href="https://dx.doi.org/10/bmch">bmch</a></cite>
							</span>
						</dt>
						<dt>
							Likelihood-free particle filter
							<span class="right-note">
								Ferrie and Granade <cite class="doi"><a href="https://dx.doi.org/10/tdj">tdj</a></cite>
							</span>
						</dt>
						<dt>
							Quantum bootstrapping
							<span class="right-note">
								Wiebe, Granade and Cory <cite class="doi"><a href="https://dx.doi.org/10/7nx">7nx</a></cite>
							</span>
						</dt>
					</dl>
				</section>

				<section data-markdown>
					### The Likelihood Function ###

					$\|\bra{+} \ee^{\ii \omega t \sigma_z / 2} \ket{+}\|^2$
					  defines probability $\Pr(d | \omega; t)$ for every
					  outcome $d$, model $\omega$ and experiment $t$.

					Basis for both maximum-likelihood and Bayesian methods.
				</section>

				<section
					data-background-image="figures/bayes-neon.jpg"
					data-background-size="110%"
					data-state="show-foot-thanks-bayes-neon"
				>
					<h3><strong>Aside:</strong> Why Bayesian?</h3>

					<p>
						Frequentist methods work. Bayesian methods also work.
						<br>
						Methodology should follow the question of interest.
					</p>

					<div class="fragment">

						<h6 class="code-caption">
							Example
						</h6>
						<blockquote>
							What should I believe the properties of my system are,
							given my experience and a set of observations?
						</blockquote>

						<p>
							Bayesian question, hence Bayesian answer.
						</p>

					</div>

				</section>

				<section data-markdown>
					### Bayesian Parameter Estimation ###

					The likelihood tells us what we learn from data:

					$$
						\Pr(\vec{x} | d; e) = \frac{\Pr(d | \vec{x}; e)}{\Pr(d | e)} \Pr(\vec{x})
					$$

					---

					Estimate $\hat{x} = \expect[\vec{x} | d; e] = \int \vec{x} \Pr(\vec{x} | d; e)\dd \vec{x}$.

					- **Optimal** for mean&mdash;squared error.
				</section>

				<section data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/bayesian-pe-flowchart.png">
				</section>

				<section>
					<h3>Inference as an Iterative Algorithm</h3>

					<p>
						<strong>Input:</strong>
							Prior $\Pr(\vec{x})$,
							data set $D$,
							likelihood $\Pr(d | \vec{x}; e)$
					</p>

					<ul style="width: 80%">
						<li>$p(\vec{x}) \gets \Pr(\vec{x})$</li>
						<li><strong>For</strong> each datum $d \in D$ and experiment $e$:
						<ul>
							<li>
								Update based on $d$:<br>
								$p(\vec{x}) \gets \Pr(d | \vec{x}; e) p(\vec{x}) / \Pr(d)$.
							</li>
						</ul>
						<li><strong>Return</strong> $\hat{x} = \int \vec{x}\,p(\vec{x})\,\dd\vec{x}$.
					</ul>

					<div class="fragment" data-markdown>
						---

						At each step, $p(\vec{x})$ also encodes our uncertainty.

						$
							\expect[(x - \hat{x})^2 | d; e] = 
	                        \mathbb{V}[x \mid d; e].
						$
					</div>
				</section>

				<section data-markdown>
					We can use our posteriors to make *adaptive* decisions:
					$$
						e_* = \operatorname{arg min}_e \expect_d[\mathbb{V}(x | d; e)]
					$$
				</section>

				<section data-markdown data-state="show-foot-how">
					### **Example**: $x = (\omega)$ ###

					Can analytically find posterior for Gaussian priors,
					use to adaptively choose $t_k$.

					![](./figures/how-t2.png)
				</section>

				<section>
					<p data-markdown>
						**Problem**: may be intractable to analytically compute $$
							\hat{x} \defeq
							\int \Pr(\vec{x} | d; e) \dd\vec{x} =
							\int \frac{
								\Pr(d | \vec{x}; e)
							}{
								\int \Pr(d | \vec{x}; e) \Pr(\vec{x}) \dd\vec{x}
							} \Pr(\vec{x}) \dd\vec{x}.
						$$
					</p>

					<div class="fragment" data-markdown>
						---

						**Answer**: numerically approximate $\int f(\vec{x}) \Pr(\vec{x} | d)\dd\vec{x}$.

						*Efficient* to use Monte Carlo integration if we can sample from
						the posterior $\Pr(\vec{x} | d; e)$.
					</div>
				</section>
<!-- 
				<section data-markdown>
					## Monte Carlo Integration ##

					$$
						\int f(\vec{x}) p(\vec{x})\dd\vec{x} \approx \frac{1}{N} \sum_i f(\vec{x}_i)
						\text{ for } \left\\{\vec{x}_i\right\\} \sim p(\vec{x})
					$$

					---

					Efficient *if* we can sample from $p(\vec{x})$.

					How do we do that for $p(\vec{x}) = \Pr(\vec{x} | d; e)$?
				</section> -->

				<section data-markdown>
					## Rejection Sampling ##

					Given samples from $\Pr(\vec{x})$ and likelihood
					function $\Pr(d | \vec{x}; e)$, how do we sample
					from posterior for datum $d$?

					- Draw $\vec{x} \sim \Pr(\vec{x})$, 
					  accept $\vec{x}$ w/ $\Pr(d | \vec{x}; e)$.

					Accepted samples are distributed according to posterior.
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="80%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/rejs-example.png"
						 >
				</section>

				<section>
					<h3>Rejection Sampling Isn't Enough</h3>

					<p>
						Let $D = {d_1, \dots, d_k}$ be a set of data.
					</p>
					<p>
						$$
							\Pr(\text{accept} | \vec{x}) = \Pr(D | \vec{x}) = \prod_{d \in D} \Pr(d | \vec{x})
							\overset{k \to \infty}{\longrightarrow} 0.
						$$
					</p>

					<hr>

					<h4><strong>Example:</strong> Biased Coin $x = (p)$</h4>
					<p>
						$\Pr(H | p) = p$, $d \in \{H, T\}$.
					</p>

					<p>
						$p \approx 0.5 \Longrightarrow \Pr(d_1, \dots, d_k | p) \approx 1 / 2^k$.
					</p>

					<p>
						We will accept exponentially few samples!
					</p>
				</section>

				<section>
					<div data-markdown>
						### Gaussian Resampling ###

						For each datum $d$, use rejection sampling to approximate posterior
						moments:

						- $\bar{x} \gets \expect[\vec{x} | d]$.
						- $\Sigma \gets \operatorname{Cov}[\vec{x} | d] = \expect[\vec{x} \vec{x}^\TT | d] - \bar{x} \bar{x}^\TT$.
					</div>

					<div data-markdown class="fragment">
						---

						At the next iteration, draw prior samples from Gaussian with these moments:
						$$
							\vec{x} \mid d \sim \mathcal{N}(\bar{x}, \Sigma)
						$$

						Keeps $\Pr(\text{accept}) \approx \text{constant}$.
					</div>
				</section>

				<section>
					<p>
						Can compute $\bar{x}$, $\Sigma$ from
						one sample at a time by accumulating
					</p>
					<p>
						$$
							x_{\Sigma} = \sum x
							\text{ and }
							x^2_{\Sigma} = \sum x^2.
						$$
					</p>

					<p>
						\begin{align}
							\bar{x} &amp; = x_{\Sigma} / n_{\text{accept}} \\
							\Sigma &amp; = x^2_{\Sigma} / n_{\text{accept}} - \bar{x}^2.
						\end{align}
					</p>

					<p>
						<em>Welford's algorithm</em>: numerically-stable modification.
					</p>
				</section>

				<section data-state="show-foot-rfpe">
					<h2>Rejection Filtering (RejF)</h2>

					<p>
					    <strong>Input:</strong>
					    Prior mean $\bar{x}$, prior covariance $\Sigma$,
						number of samples $m$ to accept.
					<p>

					<ul style="width: 80%">
						<li><strong>For</strong> each datum $d$ and experiment $e$:</li>
						<ul>
							<li>
								$n, \bar{x}', M_2 \gets 0$
								<span class="comment">Initialize Welford.</span>
							</li>
							<li><strong>While</strong> $n < m$:</li>
							<ul>
								<li>
									Draw $\vec{x} \sim \mathcal{N}(\bar{x}, \Sigma)$.
									<span class="comment">Sample f/ prior.</span>
								</li>
								<li>Accept $\vec{x}$ w/ $\Pr(d | \vec{x}; e)$.</li>
								<li><strong>If</strong> accepted, update $n$, $\bar{x}'$, $M_2$.</li>
							</ul>
							<li>
								$\bar{x} \gets \bar{x}'$, $\Sigma \gets M_2 / (n - 1)$.
								<span class="comment">
									Est. moments.
								</span>
							</li>
						</ul>
					</ul>

					<p class="fragment">
						Easy to implement and embed in control hardware.
					</p>
				</section>
<!-- 
				<section data-markdown>

					## Advantages of RejF ##

					- Accept pr based on a single datum
					  ⇒ likelihood $\not\to 0$.
					- Easy to implement
					- Never needed to remember each accepted $x$!
						- Very low-memory (constant # of
						  registers), ideal for FPGA use.
					- Easily parallelizable
					- “Reset rule” can abort from approximation failures.
				</section>
 -->
				<section data-state="show-foot-fast-pe">
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<p>
						Prepare state $\ket{\phi}$ s. t. $U\ket{\phi} = \ee^{\ii \phi}\ket{\phi}$,
						measure to learn $\phi$.
					</p>

					<img src="./figures/pe-circuit-crop.png">

					<p>
						$\Pr(1 | \phi; M, \theta) = \cos^2(M [\phi - \theta])$
					</p>

					<!-- <p>
						$\theta = 0 \Rightarrow$ freq. est likelihood, w/ $\phi = \omega$, $M = t$.
					</p> -->

					<dl style="font-size: 65%">
						<dd style="position: relative; left: -1.7em; top: 0.75em"><h4>
							Applications
						</h4></dd>

						<dt>Interferometry / metrology
							<span class="right-note">
								Higgins <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/crwd6w">crwd6w</a></cite>
							</span>
						</dt>

						<dt>Gate calibration / robust PE
							<span class="right-note">
								Kimmel <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/bmrg">bmrg</a></cite>
							</span>
						</dt>

						<dt>Quantum simulation and chemistry
							<span class="right-note">
								Reiher <em>et al.</em>
								<cite class="arxiv"><a href="https://arxiv.org/abs/1605.03590">1605.03590
								</a></cite>
							</span>
						</dt>
					</dl>
				</section>


				<section data-state="show-foot-rfpe">
					<h3><strong>Example:</strong> Phase Estimation, $x = (\phi)$</h3>

					<img src="./figures/pe-error.png" width="80%">
				</section>

				<section>
					<p  data-markdown>
						**Drawback**: RejF requires posterior after each datum
						to be $\approx$ Gaussian.
					</p>

					<p class="fragment" data-markdown>
						We can solve this by using a more general approach:

						- Weaken Gaussian assumption.
						- Generalize the rejection sampling step.
					</p>
				</section>

				<section data-state="show-foot-lw">
					<h2>Liu-West Resampler</h2>

					<p>
						If we remember each sample $\vec{x}$, we can use them to
						relax RejF assumptions.
					</p>

					<p>
						<strong>Input:</strong>
						$a, h \in [0, 1]$ s.t. $a^2 + h^2 = 1$,
						distribution $p(\vec{x})$.
					</p>

					<ul>
						<li>
							Approximate $\bar{x} \gets \expect[\vec{x}]$, $\Sigma \gets \operatorname{Cov}(\vec{x})$
						</li>
						<li>Draw <em>parent</em> $\vec{x}$ from $p(\vec{x})$.</li>
						<li>Draw $\vec{\epsilon} \sim \mathcal{N}(0, \Sigma)$.</li>
						<li>
							<strong>Return</strong>
							new sample $\vec{x}' \gets a \vec{x} + (1 - a) \bar{x} + h \vec{\epsilon}$.
						</li>
					</ul>
				</section>

				<section data-markdown>
					### Why Does Liu-West Work? ###

					\begin{align}
						\vec{x}'          &amp; \gets a \vec{x} + (1 - a) \bar{x} + h \vec{\epsilon} \\\\
						\expect[\vec{x}'] &amp; = [a + (1 - a)] \bar{x} \\\\
						\Cov(\vec{x}')    &amp; = (a^2 + h^2) \Cov(\vec{x}). \\\\
						\Longrightarrow a^2 + h^2 &amp; = 1 \text{ preserves } \expect[\vec{x}], \Cov(\vec{x}).
					\end{align}

					---

					Mixes original approximation with $1 - a$ of a Gaussian
					matching moments.

					- $a \to 0$: RejF (assumed density) approx
					- $a \to 1$: Bootstrap
					- $a = 0.98$: typical case (2% Gaussian).
				</section>

				<section data-markdown>
					### From Samples to Particles ###

					Define a particle $(w_i, \vec{x}_i)$ as
					a sample $\vec{x}_i$ and a weight $w_i \in [0, 1]$.

					- $\expect[\vec{x}] = \sum_i w_i \vec{x}_i$
					- $\Cov(\vec{x}) =
					  \sum_i w_i \vec{x}_i \vec{x}_i^\TT - \expect[\vec{x}]\expect^\TT[\vec{x}]$

					- $\expect[f(\vec{x})] = \sum_i w_i f(\vec{x}_i)$

					---

					Corresponds to
					$
						p(\vec{x}) \approx \sum_i w_i \delta(\vec{x} - \vec{x}_i).
					$
				</section>

				<section>
					<p>
						Particles can represent distributions using either<br>
						<span style="color: #D55E00;">weights</span> or
						<span style="color: #56B4E9;">positions</span>.
					</p>

					<img src="figures/impovrishment.png" class="stretch">
				</section>


				<section data-state="show-foot-smc">
					<h2>Particle Filter</h2>

					<ul>
						<li>
							Draw $N$ initial samples $\vec{x}_i$ from the prior $\Pr(\vec{x})$
							w/ uniform weights.
						</li>

						<li>
							Instead of rej. sampling, update <span style="color: #D55E00;">weights</span>
							by
							\begin{align}
								\tilde{w}_i &amp; = w_i \times \Pr(d | \vec{x}_i; e)
							\end{align}
						</li>
						<li>
							Renormalize.
							\begin{align}
								w_i &amp; \mapsto \tilde{w}_i / \sum_i \tilde{w}_i
							\end{align}
						</li>

						<li>
							Periodically use Liu-West to draw new $\{\vec{x}_i\}$
					  		with uniform weights.
					  		<span class="comment">
					  			Store posterior in <span style="color: #56B4E9;">positions</span>.
					  		</span>
					  	</li>
					</ul>
				</section>

				<section data-background="#000" data-background-video="figures/multicos-distributions.mp4">					
				</section>


				<section>
					<h4>Useful for Hamiltonian models...</h4>

					<dl style="font-size: 80%" class="border-dl">
						<dt>Rabi/Ramsey/Phase est.
							<span class="right-note">
								Granade <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>
							</span class="right-note">
						</dt>

						<dt>Swap spectroscopy
							<span class="right-note">
								Stenberg <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/7nw">7nw</a></cite>
							</span>
						</dt>
					</dl>

					<h4 style="padding-top: 0.5em;">
						...as well as other QIP tasks.
					</h4>

					<dl style="font-size: 80%" class="border-dl">
						<dt>Tomography
							<span class="right-note">
								Huszár and Holsby
								<cite class="doi"><a href="https://dx.doi.org/10/s86">s86</a></cite>
							</span><br>&nbsp;
							<span class="right-note">					
								Struchalin <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/bmg5">bmg5</a></cite>
							</span><br>&nbsp;
							<span class="right-note">					
								Ferrie
								<cite class="doi"><a href="https://dx.doi.org/10/7nt">7nt</a></cite>							
							</span><br>&nbsp;
							<span class="right-note">
								Granade <em>et al.</em>						
								<cite class="doi"><a href="https://dx.doi.org/10/bhdw">bhdw</a></cite>,
								<cite class="arxiv"><a href="https://arxiv.org/abs/1605.05039">1605.05039</a></cite>
							</span>
						</dt>

						<dt>Randomized benchmarking
							<span class="right-note">
								Granade <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/zmz">zmz</a></cite>
							</span>
						</dt>

						<dt>Continuous measurement
							<span class="right-note">
								Chase and Geremia
								<cite class="doi"><a href="https://dx.doi.org/10/chk4q7">chk4q7</a></cite>
							</span>
						</dt>

						<dt>Interferometry/metrology
							<span class="right-note">
								Granade
								<cite class="hdl"><a href="https://dx.doi.org/10012/9217"><span>10012/</span>9217</a></cite>
							</span>
						</dt>
					</dl>
				</section>

<!-- 
				<section data-markdown data-state="show-foot-rohl">
					### **Example**: $\vec{x} = (\omega, T_2)$ ###

					![](figures/unknown-t2.png)
				</section>

				<section data-state="show-foot-thesis">
					<h3><strong>Example</strong>: $\vec{x} =$ COSY / Crosstalk</h3>

					<p>\(
						H(\omega_1, \omega_2, J) = \frac12 \left(
							\omega_1 \sigma_z \otimes \id + 
							\omega_2 \id \otimes \sigma_z +
							J \sigma_z \otimes \sigma_z
						\right)
					\)</p>

					<img src="figures/cosy-pulses.png">
				</section>

				<section data-state="show-foot-thesis">
					<h3><strong>Example</strong>: $\vec{x} =$ COSY / Crosstalk</h3>

					<img src="figures/cosy-loss.png" width="80%">
				</section>
-->
				<section>
					<h3>Estimation in Practice</h3>

					<p>
						We need a bit more to make particle filtering a
						<em>practical</em> solution.
					</p>
					<ul style="width: 80%">
						<li>
							Error bars
							<span class="right-note">
								How well do we know $\vec{x}$?
							</span>
						</li>

						<li>
							Time-dependence
							<span class="right-note">
								$\vec{x} = \vec{x}(t)$
							</span>
						</li>

						<li>
							Software impl.
							<span class="right-note">
								Off-the-shelf.
							</span>
						</li>
					</ul>

					<p><em>
						Dealing with each in turn...
					</em></p>
				</section>

				<section data-state="show-foot-thanks-hpd">
					<h3>Error Bars</h3>

					<p>
						Particle filters report <em>credible regions</em> $X_\alpha$ s.t.
						$$
							\Pr(\vec{x} \in X_\alpha | d; e) \ge \alpha.
						$$
					</p>

					<img src="figures/nc.png" class="stretch">

					<p>
						E.g.: Posterior covariance ellipse, convex hull, MVEE.
					</p>

				</section>

				<section data-state="show-foot-condensation">
					<h3>Time-Dependence</h3>

					<p>
						In addition to updating particle weights, move each particle
						stochastically:
					</p>
					<p>
						$$
							\vec{x}(t_{k+1}) = \vec{x}(t_k) + \vec{\eta},\qquad
							\vec{\eta} \sim \mathcal{N}(0, (t_{k+1} - t_k) \Sigma)
						$$
					</p>

					<img class="stretch" src="figures/diffusive-coin.png">
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="80%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/rabi-random-walk.png"
						 >
				</section>

				<section data-state="show-foot-qinfer">
					<div data-markdown>
						### Software Implementation ###

						We provide **QInfer**, an open-source implementation for use in quantum information.

						Written in Python, works with MATLAB and Julia.
					</div>

					<!-- <div class="fragment" data-markdown>
						- - -

						Puts focus on algorithms and experiments, not implementations.
					</div> -->

 					<div class="fragment">
						<hr>

						<h6 class="code-caption">
							Installation
						</h6>
						<pre><code data-trim style='font-size: 140%;' class="bash">
$ pip install qinfer
						</code></pre>
					</div>
				</section>

				<section>
						<h6 class="code-caption">
							Updater Loop
						</h6>
						<pre><code data-trim class='python stretch' style="font-size: 130%">
>>> updater = SMCUpdater(model, n_particles, prior)
>>> heuristic = heuristic_class(updater)
>>> for idx in range(n_measurements):
...     experiment = heuristic()
...     datum = model.simulate_experiment(
...         true_model, experiment
...     )
...     updater.update(datum, experiment)
>>> est = updater.est_mean()
						</code></pre>

						<p>
							<code>SMCUpdater</code> formalizes particle filtering
							as an <em>iterative</em> algorithm.
						</p>
				</section>

				<section>
					<h3><strong>QInfer Example:</strong> RB, $\vec{x} = (p, A, B)$</h3>


					<div>
						<h6 class="code-caption">
							Setup
						</h6>
						<pre><code data-trim class='python' style="font-size: 110%">
>>> import numpy as np
>>> import qinfer as qi
						</code></pre>
					</div>

					<div class="fragment">
						<h6 class="code-caption">
							Simplified Estimation
						</h6>
						<pre><code data-trim class="python">
>>> data = np.column_stack([
...     counts, seq_lengths, n_shots * np.ones_like(counts)
... ])
>>> mean, cov = qi.simple_est_rb(
...     data, n_particles=12000, p_min=0.8
... )
						</code></pre>
					</div>
				</section>

				<section>
					<h3><strong>QInfer Example:</strong> RB, $\vec{x} = (p, A, B)$</h3>

					<p>
						Can also customize model, prior, and updater loop.
					</p>

					<div class="fragment">
						<h6 class="code-caption">
							Model
						</h6>
						<pre><code data-trim class='python' style="font-size: 110%">
>>> model = qi.BinomialModel(
...     qi.RandomizedBenchmarkingModel()
... )
						</code></pre>
					</div>					

					<div class="fragment">
						<h6 class="code-caption">
							Prior
						</h6>
						<pre><code data-trim class='python' style="font-size: 110%">
>>> prior = qi.PostselectedDistribution(
...     qi.ProductDistribution(
...         # Use the same prior over $p$.
...         qi.UniformDistribution([0.8, 1]),
...         # Tight prior over $A,\,B$.
...         qi.MultivariateNormalDistribution(
...             np.array([0.498, 0.499]),
...             np.diag([0.004, 0.002]) ** 2
...         )
...     ), model)
						</code></pre>
					</div>
				</section>

				<section>
					
					<div>						
						<h6 class="code-caption">
							Updater Loop
						</h6>
						<pre><code data-trim class="python" style="overflow-x: hidden;"">
>>> updater = qi.smc.SMCUpdater(model, 8000, prior)
>>> for idx_exp in range(n_exp):
...     expparams = np.array(
...         [(seq_lengths[idx_exp], n_shots)],
...         dtype=model.expparams_dtype)
...     updater.update(counts[idx_exp], expparams)
						</code></pre>
					</div>
				</section>

 				<section data-markdown
						 data-background="#000"
				         data-background-size="100%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/rb-custom-updater.png"
						 >
				</section>

				<section>
					<h3><strong>QInfer:</strong> Assessing Performance</h3>

					<dl>
						<dt>
							Risk: <span class="right-note">
								average error <strong>given</strong> $\color{white}{\vec{x}}$
							</span>
						</dt>

						<dt>
							Bayes risk: <span class="right-note">
								average error <strong>over</strong> $\color{white}{\vec{x}}$
							</span>
						</dt>
					</dl>

					<div class="fragment">
						<hr>

						<ul>
							<li>Exact $\hat{x}$ optimal for Bayes risk / squared error.</li>
							<li>Difficult to <em>prove</em>; can find numerically.</li>
				  		</ul>


						<h6 class="code-caption">
							Performance Testing
						</h6>
						<pre><code class="python" data-trim>
>>> perf = perf_test_multiple(
...     n_trials=50, n_exp=50, n_particles=1000,
...     model=SimplePrecessionModel(),
...     prior=UniformDistribution([0, 1]),
...     heuristic_class=ExpSparseHeuristic
... )
						</code></pre>
					</div>
				</section>

				<section>
					<h2>Bigger and Better</h2>

					<p>
						We've seen that filtering is useful for estimating
						small quantum models. Now let's push on to bigger systems.
					</p>
				</section>

				<section>
					<p>
						What challenges do we face for large systems?
					</p>
					
					<div class="fragment">
						<h3>Simulation Costs</h3>

						<p style="font-size: 140%">
							\begin{align}
								\tilde{w}_i &amp; = w_i \times \color{red}{\Pr(d | \vec{x}_i; e)} \\
								w_i &amp; \mapsto \tilde{w}_i / \sum_i \tilde{w}_i
							\end{align}
						</p>

						<div data-markdown>
							- $\Pr(d | \vec{x}_i; e)$ is a *simulation*.
							- Need to
							  simulate for each particle (~1000s calls/datum).
							- Infeasible for large quantum models.

							Let's do better: use *quantum* simulation instead.
						</div>
					</div>
				</section>

				<section data-markdown>
					### Two Kinds of Simulation ###

					![](figures/simulators.png)

					Weak simulators produce *plausible datasets*.
				</section>

				<section>
					<div data-markdown>
						### Likelihood-Free RejF ###

						Replace rej. sampling step by drawing
						datum from likelihood instead of computing
						exact value:

						- Draw datum $d'$ from $\Pr(d | \vec{x}; e)$.
						- Accept $\vec{x}$ if $d = d'$.
					</div>

					<img class="stretch" src="figures/lfrf.png">
				</section>

				<section data-markdown data-state="show-foot-lfpe">
					### Likelihood-Free Particle Filtering ###

					Can also use frequencies $f$ from weak simulation to
					approximate likelihoods
					in particle filtering.

					$$
						\widehat{\Pr}(d | \vec{x}; e) =
							\frac{f(d | \vec{x}; e)}{k}
					$$

					$k$: number of weak simulation calls used.
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak-desaturated.png"
						 >
					### **Example**: Noisy Coin ###

					How well can we learn the bias $x = (p)$ of a noisy coin?

					$$
						\Pr(\text{click} | p) = 0.95 p + 0.1 (1 - p)
					$$
				</section>

				<section data-markdown
						 data-background="#000"
				         data-background-size="60%"
				         data-background-position="center"
				         data-background-repeat="no-repeat"
						 data-background-image="figures/lfpe-vs-n-weak.png"
						 data-background-transition="fade-in"
						 data-transition="fade-in">
				</section>

				<!-- 
				<section data-markdown data-state="show-foot-qhl">
					### Quantum Hamiltonian Learning ###

					![](figures/qle.png)
				</section>

				<section data-markdown>
					### **Example**: Nearest-Neighbor 1D Ising ###

					![](figures/qle-line-ising.png)
				</section>
				-->

				<section data-markdown>
					### Quantum Hamiltonian Learning ###

					We can learn large Hamiltonians by
					using likelihood-free filtering w/ interactivity.
					
					![](figures/iqle.png)

					Perform weak simulations on trusted device only.
				</section>

				<section>
					<h3><strong>Example:</strong> Ising on Complete Graph</h3>

					<img src="figures/iqle-complete-ising.png" class="stretch">
				</section>

				<section data-markdown data-state="show-foot-qhl-noise">
					Robust even to wrong model. ($0.5$ NN + $10^{-4}$ Complete)

					![](figures/qhl-bad-model.png)
				</section>

				<section data-state="show-foot-qbs">
					<h3>Quantum Bootstrapping</h3>

					<p>
						One important approximation f/ physical insight:<br>
						<em>information locality</em>.
					</p>

					<div class="fragment">
						<p>
							Allows using <em>small</em> trusted device to
							learn large Hamiltonians.
						</p>
					</div>

				</section>

				<section>
					<img src="./figures/qbs-lightcones.png" class="stretch">

					<p>
						Approximation quality can be bounded if
						Lieb-Robinson velocity is finite.
					</p>
				</section>

				<section>
					<p>
						Scan trusted device across untrusted.
					</p>

					<img src="./figures/scanning.png" width="55%">

					<p>
						Run particle filter <em>only</em> on supported
						parameters.
					</p>
				</section>

				<section>
					<h4>50 qubit Ising chain, 8 qubit simulator, 4 qubit observable</h4>

					<img src="./figures/qbs-error-per-scan.png" width="60%">
				</section>

				<section>
					<h3>
						Filtering
					</h3>

					<ul>
						<li>
							Practical solution for
							current experimental tasks.
						</li>

						<li>
							Enables learning large Hamiltonians using quantum
							resources.
						</li>

						<li>
							Physical insight gives new statistical
							algorithm for even larger systems.
						</li>
					</ul>
				</section>

				<section>
					<h3>Going Further</h3>

					<dl class="border-dl">
						<dt>Hyperparameterization
							<span class="right-note">
								Granade <em>et al.</em>
								<cite class="doi"><a href="https://dx.doi.org/10/s87">s87</a></cite>
							</span>
						</dt>
						<dd>
							$\Pr(d | y) = \expect_x[\Pr(d | x) \Pr(x | y)]$.<br>
							Allows composing w/ noise, inhomogeneity, etc.
						</dd>

						<!-- <dt>Time-dependence
							<span class="right-note">
								Isard and Blake
								<cite class="doi"><a href="https://dx.doi.org/10/cc76f6">cc76f6</a></cite>
							</span>
						</dt>
						<dd>
							Adding timestep update allows learning stochastic
							processes.
						</dd> -->

						<dt>Model selection							
							<span class="right-note">
								Ferrie
								<cite class="doi"><a href="https://dx.doi.org/10/7nt">7nt</a></cite>
							</span>
						</dt>
						<dd>
							Using acceptance ratio or normalizations
							enables comparing models.
						</dd>
					
						<dt>Quantum filtering
							<span class="right-note">
								Wiebe and Granade
								<cite class="arxiv"><a href="http://arxiv.org/abs/1512.03145">1512.03145</a></cite>
							</span>
						</dt>
						<dd>
							Rejection filtering is a dequantization of
							quantum filtering using
							Harrow <em>et al.</em>
							<cite class="doi"><a href="https://dx.doi.org/10/bcz3hc">bcz3hc</a></cite>.
						</dd>
					</dl>
				</section>

				<section>
					<h1 class="thanks">Thank you!</h1>
				</section>

				<section>
					<h2>Welford's Algorithm</h2>
					
					<p>
						Can compute $\bar{x}$, $\Sigma$ from
						one sample at a time. Numerically stable.
					</p>

					<ul style="width: 80%">
						<li>$n, \bar{x}, M_2 \gets 0$.</li>
						<li><strong>For</strong> each sample $x$:</li>
						<ul>
							<li>
								$n \gets n + 1$
								<span class="comment">Record # of samples</span>
							</li>
							<li>
								$\Delta \gets x - \mu$
								<span class="comment">Diff to running mean</span>
							</li>
							<li>
								$\bar{x} \gets \bar{x} + \Delta / n$
								<span class="comment">Update running mean</span>
							</li>
							<li>
								$M_2 \gets M_2 + \Delta (x - \bar{x})$
								<span class="comment">Update running var</span>
							</li>
						</ul>
						<li><strong>Return</strong> mean $\bar{x}$, variance $M_2 / (n - 1)$.</li>
					</ul>

					<p>
						Vector case is similar.
					</p>
				</section>

				<section>
					<p>
						We design experiments using the
					</p>

					<h3><strong>PGH</strong>: Particle Guess Heuristic</h3>

					<ul>
						<li>
							Draw $\vec{x}_-, \vec{x}_-'$ from current
					  		posterior.
					  	</li>
						<li>Let $t = 1 / |\vec{x}_- - \vec{x}_-'|$.</li>
						<li><strong>Return</strong> $e = (\vec{x}_-, t)$.</li>
					</ul>

					<div class="fragment">
						<hr>

						<p>
							Adaptively chooses experiments such that<br>
							$t |\vec{x}_- - \vec{x}_-'| \approx\,$ constant.
						</p>
					</div>
				</section>




			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
		    'HTML-CSS': {
				preferredFont: null,
				webFont: 'Gyre-Pagella'
			},
			TeX: {
				Macros: {
					ee: "{\\mathrm{e}}",
					ii: "{\\mathrm{i}}",
					dd: "{\\mathrm{d}}",
					id: "{\u{1d7d9}}",
					TT: "{\\mathrm{T}}",
					defeq: "{\\mathrel{:=}}",
					Tr: "{\\operatorname{Tr}}",
					Var: "{\\operatorname{Var}}",
					Cov: "{\\operatorname{Cov}}",
					rank: "{\\operatorname{rank}}",
					expect: "{\\mathbb{E}}",
					sket: ["{|#1\\rangle\\negthinspace\\rangle}", 1],
					sbraket: ["{\\langle\\negthinspace\\langle#1\\rangle\\negthinspace\\rangle}", 1],
					Gini: "{\\operatorname{Ginibre}}",
					supp: "{\\operatorname{supp}}",
					ket: ["{\\left|#1\\right\\rangle}", 1],
					bra: ["{\\left\\langle#1\\right|}", 1],
					braket: ["{\\left\\langle#1\\right\\rangle}", 1]
				}
			}
		  });
		</script>
		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: true,
				zoomKey: 'shift',

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: '//cdnjs.cloudflare.com/ajax/libs/socket.io/0.9.16/socket.io.min.js', async: true },
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
